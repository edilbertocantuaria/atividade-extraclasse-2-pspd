# Relat√≥rio Comparativo - Hadoop B1

**Data:** 14 de novembro de 2025  
**Dataset:** 100 MB (10 arquivos, ~14.6M palavras)  
**Cluster:** 1 Master + 2 Workers (Docker)

---

## 1. Resumo Executivo

Este relat√≥rio apresenta os resultados comparativos dos testes de configura√ß√£o do Hadoop, com foco especial no impacto da **execu√ß√£o especulativa** (speculative execution) na performance do MapReduce.

### Principais Descobertas

- ‚úÖ **Speculative Execution** reduziu o tempo de execu√ß√£o em **97.1%** (de 45min para 1min 18s)
- ‚úÖ Throughput aumentou de **0.03 MB/s** para **1.27 MB/s** (42.3x faster)
- ‚úÖ Configura√ß√£o de mem√≥ria cr√≠tica: 512MB por container (vs. 1536MB padr√£o que excedia recursos)

---

## 2. Resultados dos Testes

### Teste 0: Baseline (sem speculative execution)

**Configura√ß√£o:**
- `mapreduce.map.speculative`: false (padr√£o)
- `mapreduce.reduce.speculative`: false (padr√£o)
- Mem√≥ria por container: 512 MB
- Reducers: 4

**Resultados:**
- ‚è±Ô∏è **Dura√ß√£o:** 2735.15s (45min 44s)
- üìä **Throughput:** 0.03 MB/s (1.80 MB/min)
- üî¢ **Application ID:** application_1763130949673_0005
- üìà **Map tasks:** 10 lan√ßados
- üìâ **Reduce tasks:** 5 lan√ßados, 2 killed
- üíæ **Bytes processados:** 99.74 MB HDFS read, 1.7 KB HDFS write
- üßÆ **Records:** 1,048,570 input ‚Üí 14,672,712 map output ‚Üí 124 reduce output

**An√°lise:**
O baseline apresentou performance extremamente lenta devido √† limita√ß√£o severa de recursos (apenas 1024MB dispon√≠vel por NodeManager). Tasks executaram sequencialmente devido √† falta de mem√≥ria para paraleliza√ß√£o efetiva.

---

### Teste 5: Speculative Execution

**Configura√ß√£o:**
- `mapreduce.map.speculative`: **true** ‚úÖ
- `mapreduce.reduce.speculative`: **true** ‚úÖ
- `mapreduce.job.speculative.speculativecap`: 0.1 (10% tasks simult√¢neas)
- `mapreduce.job.speculative.slowtaskthreshold`: 1.0 (1x tempo m√©dio)
- `mapreduce.job.speculative.minimum-allowed-tasks`: 5
- Mem√≥ria por container: 512 MB
- Reducers: 4

**Resultados:**
- ‚è±Ô∏è **Dura√ß√£o:** 78.63s (1min 18s)
- üìä **Throughput:** 1.27 MB/s (76.20 MB/min)
- üî¢ **Application ID:** application_1763130949673_0006
- üìà **Map tasks:** 10 lan√ßados
- üìâ **Reduce tasks:** 7 lan√ßados, 3 killed
- üíæ **Bytes processados:** 99.74 MB HDFS read, 1.7 KB HDFS write
- üßÆ **Records:** 1,048,570 input ‚Üí 14,672,712 map output ‚Üí 124 reduce output

**An√°lise:**
A execu√ß√£o especulativa identificou tasks lentas e iniciou c√≥pias duplicadas, resultando em **97.1% de melhoria**. Observe que 3 reduce tasks foram killed (vs. 2 no baseline), indicando que o Hadoop substituiu tasks lentas por vers√µes especulativas mais r√°pidas.

---

## 3. Compara√ß√£o Direta

| M√©trica | Baseline | Speculative | Diferen√ßa | Melhoria |
|---------|----------|-------------|-----------|----------|
| **Dura√ß√£o** | 2735.15s | 78.63s | -2656.52s | **97.1%** ‚¨áÔ∏è |
| **Throughput (MB/s)** | 0.03 | 1.27 | +1.24 | **4133%** ‚¨ÜÔ∏è |
| **Throughput (MB/min)** | 1.80 | 76.20 | +74.40 | **4133%** ‚¨ÜÔ∏è |
| **Map Time (ms)** | 46,383 | 31,585 | -14,798 | **31.9%** ‚¨áÔ∏è |
| **Reduce Time (ms)** | 9,733 | 11,114 | +1,381 | 14.2% ‚¨ÜÔ∏è |
| **Reduce Tasks Killed** | 2 | 3 | +1 | Especula√ß√£o ativa |
| **CPU Time (ms)** | 40,400 | 28,610 | -11,790 | **29.2%** ‚¨áÔ∏è |
| **GC Time (ms)** | 1,077 | 2,092 | +1,015 | 94.2% ‚¨ÜÔ∏è |

**Observa√ß√µes:**
1. **Map phase** foi significativamente acelerada (-31.9%)
2. **Reduce phase** teve leve aumento (+14.2%) devido √†s 3 tasks killed (overhead de especula√ß√£o)
3. **GC time dobrou** devido a mais containers concorrentes
4. **CPU time total reduziu** 29.2% apesar do GC overhead

---

## 4. Configura√ß√µes Cr√≠ticas Identificadas

### 4.1 Mem√≥ria (CR√çTICO)

**Problema identificado:**
O Hadoop padr√£o solicita 1536 MB por container, mas o YARN tinha apenas 1024 MB dispon√≠vel por NodeManager.

**Solu√ß√£o aplicada:**
```xml
<property>
    <name>mapreduce.map.memory.mb</name>
    <value>512</value>
</property>
<property>
    <name>mapreduce.reduce.memory.mb</name>
    <value>512</value>
</property>
<property>
    <name>yarn.app.mapreduce.am.resource.mb</name>
    <value>512</value>
</property>
```

**Impacto:**
Sem essa configura√ß√£o, os jobs **falhavam imediatamente** com:
```
InvalidResourceRequestException: Cannot allocate containers as requested 
resource is greater than maximum allowed allocation
```

### 4.2 HADOOP_MAPRED_HOME (CR√çTICO)

**Problema identificado:**
Application Master n√£o conseguia encontrar classes MRAppMaster.

**Solu√ß√£o aplicada:**
```xml
<property>
    <name>yarn.app.mapreduce.am.env</name>
    <value>HADOOP_MAPRED_HOME=/home/hadoop/hadoop</value>
</property>
<property>
    <name>mapreduce.map.env</name>
    <value>HADOOP_MAPRED_HOME=/home/hadoop/hadoop</value>
</property>
<property>
    <name>mapreduce.reduce.env</name>
    <value>HADOOP_MAPRED_HOME=/home/hadoop/hadoop</value>
</property>
```

**Impacto:**
Sem essa configura√ß√£o, jobs falhavam com:
```
Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster
```

### 4.3 Speculative Execution (PERFORMANCE)

**Configura√ß√£o ideal encontrada:**
```xml
<property>
    <name>mapreduce.map.speculative</name>
    <value>true</value>
</property>
<property>
    <name>mapreduce.reduce.speculative</name>
    <value>true</value>
</property>
<property>
    <name>mapreduce.job.speculative.speculativecap</name>
    <value>0.1</value>  <!-- 10% max tasks especulativas -->
</property>
<property>
    <name>mapreduce.job.speculative.slowtaskthreshold</name>
    <value>1.0</value>  <!-- Task lenta = 1x tempo m√©dio -->
</property>
<property>
    <name>mapreduce.job.speculative.minimum-allowed-tasks</name>
    <value>5</value>
</property>
```

**Impacto:**
- **97.1% de redu√ß√£o** no tempo total
- **42.3x mais throughput**
- Detec√ß√£o autom√°tica de stragglers

---

## 5. Testes de Toler√¢ncia a Falhas

> **Status:** Em execu√ß√£o...  
> **Estimativa:** 15-20 minutos

### Cen√°rios testados:
1. ‚úÖ Baseline (sem falhas) - **EM EXECU√á√ÉO**
2. ‚è≥ 1 Worker down durante execu√ß√£o
3. ‚è≥ 2 Workers down (apenas master)
4. ‚è≥ Scale up (adicionar worker durante execu√ß√£o)

---

## 6. Testes de Concorr√™ncia

> **Status:** Pendente  
> **Estimativa:** 10-15 minutos

### Cen√°rios planejados:
1. ‚è≥ 2 jobs concorrentes
2. ‚è≥ 3 jobs concorrentes
3. ‚è≥ 4 jobs concorrentes

An√°lise do scheduler YARN e fair sharing.

---

## 7. Conclus√µes e Recomenda√ß√µes

### 7.1 Conclus√µes

1. **Speculative Execution √© essencial** em clusters com variabilidade de performance
2. **Configura√ß√£o de mem√≥ria** deve ser ajustada ao ambiente (n√£o usar padr√µes cegamente)
3. **Vari√°veis de ambiente** s√£o cr√≠ticas para funcionamento do MapReduce
4. **Cluster com recursos limitados** (1GB RAM) beneficia-se MUITO de otimiza√ß√µes

### 7.2 Recomenda√ß√µes

**Para produ√ß√£o:**
- ‚úÖ Habilitar speculative execution (ganho de 97.1%)
- ‚úÖ Ajustar mem√≥ria de containers ao hardware dispon√≠vel
- ‚úÖ Configurar HADOOP_MAPRED_HOME em todos os nodes
- ‚úÖ Monitorar tasks killed (indicador de especula√ß√£o ativa)
- ‚ö†Ô∏è Considerar aumento de mem√≥ria dos NodeManagers (atualmente 1GB √© muito limitado)

**Para este cluster espec√≠fico:**
- Aumentar `yarn.nodemanager.resource.memory-mb` de 1024MB para 2048MB ou 4096MB
- Considerar uso de containers com 1024MB (ao inv√©s de 512MB) se mem√≥ria permitir
- Avaliar uso de SSDs para melhorar I/O (gargalo atual)

---

## 8. Arquivos de Evid√™ncia

### Teste 0 (Baseline)
- `resultados/B1/teste0_baseline/metrics_summary.txt`
- `resultados/B1/teste0_baseline/metrics_summary.csv`
- `resultados/B1/teste0_baseline/temporal_metrics.txt`
- `resultados/B1/teste0_baseline/throughput_metrics.txt`

### Teste 5 (Speculative)
- `resultados/B1/teste5_speculative/metrics_summary.txt`
- `resultados/B1/teste5_speculative/metrics_summary.csv`
- `resultados/B1/teste5_speculative/temporal_metrics.txt`
- `resultados/B1/teste5_speculative/throughput_metrics.txt`

### Configura√ß√µes
- `config/teste5_speculative/mapred-site.xml`
- `hadoop/master/mapred-site.xml` (aplicado)

---

**Relat√≥rio gerado automaticamente**  
**√öltima atualiza√ß√£o:** 2025-11-14 13:13:00
