# ğŸš€ Guia de ExecuÃ§Ã£o no Google Colab

## MÃ©todo 1: Upload Direto (Mais Simples)

1. **Acesse o Colab:**
   ```
   https://colab.research.google.com/
   ```

2. **Upload do Notebook:**
   - Clique em `File` â†’ `Upload notebook`
   - Selecione o arquivo `B2_pipeline.ipynb`
   - OU arraste e solte o arquivo na janela

3. **Execute:**
   - `Runtime` â†’ `Run all` (Ctrl+F9)
   - Aguarde ~3-5 minutos para instalaÃ§Ã£o de dependÃªncias
   - VisualizaÃ§Ãµes aparecerÃ£o automaticamente inline

## MÃ©todo 2: Direto do GitHub

1. **Link direto:**
   ```
   https://colab.research.google.com/github/edilbertocantuaria/atividade-extraclasse-2-pspd/blob/main/spark/notebooks/B2_pipeline.ipynb
   ```

2. **Ou via interface:**
   - Acesse `https://colab.research.google.com/`
   - Clique na aba `GitHub`
   - Cole a URL: `https://github.com/edilbertocantuaria/atividade-extraclasse-2-pspd`
   - Selecione `B2_pipeline.ipynb`

3. **Execute todas as cÃ©lulas:**
   - `Runtime` â†’ `Run all`

## â±ï¸ Tempo de ExecuÃ§Ã£o Esperado

| Fase | DuraÃ§Ã£o | DescriÃ§Ã£o |
|------|---------|-----------|
| InstalaÃ§Ã£o | 2-3 min | pip install pyspark, kafka, wordcloud |
| Download Kafka | 1-2 min | Download de ~100MB |
| InicializaÃ§Ã£o | 30-60s | Spark session + Kafka broker |
| Processamento | 2-3 min | Producer + Streaming + Consumer |
| **TOTAL** | **6-9 min** | ExecuÃ§Ã£o completa |

## ğŸ“Š O Que Esperar

### SaÃ­das Esperadas:

1. **Mensagens de progresso:**
   ```
   âœ… DependÃªncias Python instaladas
   âœ… Kafka extraÃ­do em /tmp/kafka_2.12-3.6.0
   âœ… SparkSession criada com sucesso
   âœ… TÃ³pico social-input criado com 3 partiÃ§Ãµes
   âœ… Producer iniciado
   âœ… Query Spark iniciada: streaming_wordcount
   ```

2. **VisualizaÃ§Ãµes (aparecem automaticamente):**
   - ğŸ“Š GrÃ¡fico de barras horizontal (Top 20-25 palavras)
   - â˜ï¸ Nuvem de palavras colorida (100-150 termos)
   - ğŸ“ˆ GrÃ¡ficos atualizados ao longo do tempo

3. **EstatÃ­sticas finais:**
   ```
   ğŸ“Š EstatÃ­sticas Finais:
   - Mensagens produzidas: ~120-150
   - Palavras Ãºnicas: ~80-100
   - Total de contagens: ~1500-2000
   - Taxa: ~1.2 rows/sec
   ```

## âœ… Checklist de ValidaÃ§Ã£o

Execute estas verificaÃ§Ãµes durante a execuÃ§Ã£o:

- [ ] CÃ©lula 1-4: InstalaÃ§Ãµes sem erros
- [ ] CÃ©lula 8-9: SparkSession criada (sem warnings crÃ­ticos)
- [ ] CÃ©lula 10-11: Kafka broker ativo (porta 9092)
- [ ] CÃ©lula 12-13: Producer iniciado e produzindo mensagens
- [ ] CÃ©lula 16-20: Query Spark em estado "ACTIVE"
- [ ] CÃ©lula 21-26: VisualizaÃ§Ãµes aparecem e atualizam
- [ ] CÃ©lula 27-29: Cleanup sem erros

## ğŸ› Troubleshooting

### Problema 1: "Java not found"
**SoluÃ§Ã£o:** Google Colab jÃ¡ tem Java instalado. Se aparecer este erro:
```python
# Execute esta cÃ©lula ANTES de criar SparkSession:
!apt-get install -y openjdk-11-jdk-headless
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
```

### Problema 2: Kafka nÃ£o inicia
**Sintoma:** `Connection refused (localhost:9092)`

**SoluÃ§Ã£o:**
```python
# Adicione delays maiores apÃ³s iniciar Kafka:
import time
time.sleep(10)  # Aumentar de 5s para 10s
```

### Problema 3: GrÃ¡ficos nÃ£o aparecem
**SoluÃ§Ã£o:**
```python
# Adicione no inÃ­cio do notebook:
%matplotlib inline
import matplotlib
matplotlib.use('Agg')
```

### Problema 4: "Out of memory"
**Causa:** Spark consumindo muita RAM

**SoluÃ§Ã£o:** Reduzir configuraÃ§Ãµes na cÃ©lula de SparkSession:
```python
spark = SparkSession.builder \
    .config("spark.driver.memory", "1g") \  # Reduzir de 2g para 1g
    .config("spark.executor.memory", "1g") \
    # ...
```

### Problema 5: Timeout em downloads
**SoluÃ§Ã£o:**
```python
# Usar espelho alternativo do Kafka:
kafka_url = "https://dlcdn.apache.org/kafka/3.6.0/kafka_2.12-3.6.0.tgz"
```

## ğŸ” ValidaÃ§Ã£o de Resultados

### Como verificar se funcionou:

1. **Producer ativo:**
   ```python
   # Deve imprimir mensagens a cada ~1.5s
   ğŸ“¤ Produzindo mensagem 1/120...
   ğŸ“¤ Produzindo mensagem 2/120...
   ```

2. **Spark processando:**
   ```python
   # Console sink deve mostrar:
   -------------------------------------------
   Batch: 0
   -------------------------------------------
   +----------+-----+
   |word      |count|
   +----------+-----+
   |spark     |15   |
   |dados     |12   |
   |hadoop    |10   |
   +----------+-----+
   ```

3. **Consumer recebendo:**
   ```python
   # WordCloudVisualizer deve imprimir:
   ğŸ“Š Recebidas 25 palavras Ãºnicas
   ğŸ“Š Recebidas 48 palavras Ãºnicas
   ```

4. **GrÃ¡ficos renderizados:**
   - Barra horizontal com cores gradientes
   - WordCloud com fundo branco e palavras coloridas

## ğŸ“ Dicas de Uso

### Modo Interativo:
- Execute cÃ©lulas uma a uma (Shift+Enter) para acompanhar cada etapa
- Ãštil para debugging e entendimento do pipeline

### Modo Automatizado:
- `Runtime` â†’ `Run all` para execuÃ§Ã£o completa
- Ãštil para demonstraÃ§Ãµes e testes rÃ¡pidos

### Salvar Resultados:
```python
# Adicionar no final do notebook:
from google.colab import files

# Baixar grÃ¡ficos
files.download('/tmp/wordcloud.png')
files.download('/tmp/barchart.png')
```

### Aumentar Dados:
```python
# Modificar na cÃ©lula do Producer:
self.num_messages = 200  # Aumentar de 120 para 200
self.interval = 1.0      # Reduzir de 1.5 para 1.0
```

## ğŸ¯ Objetivos de Aprendizado

Ao final da execuÃ§Ã£o, vocÃª terÃ¡ demonstrado:

âœ… **Setup de ambiente Big Data** (Spark + Kafka)  
âœ… **Streaming em tempo real** (Structured Streaming)  
âœ… **Pipeline completo** (Producer â†’ Processing â†’ Consumer)  
âœ… **VisualizaÃ§Ã£o de dados** (WordCloud + Charts)  
âœ… **Monitoramento** (Spark UI, Kafka offsets)  
âœ… **Cleanup de recursos** (Stop graceful de serviÃ§os)

## ğŸ“ Suporte

Se encontrar problemas nÃ£o listados acima:

1. Reinicie o runtime: `Runtime` â†’ `Restart runtime`
2. Execute novamente: `Runtime` â†’ `Run all`
3. Verifique os logs de cada cÃ©lula
4. Consulte o README.md principal para detalhes tÃ©cnicos

---

**Boa execuÃ§Ã£o! ğŸš€**
