{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cad6fbc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Justificativa: Alternativa ao Discord\n",
    "\n",
    "### Por que n√£o usar Discord?\n",
    "\n",
    "A integra√ß√£o direta com Discord apresenta diversos desafios t√©cnicos que a tornam **invi√°vel** para execu√ß√£o em ambiente de notebook:\n",
    "\n",
    "#### 1.1 Complexidade de Autentica√ß√£o OAuth\n",
    "- Discord exige cria√ß√£o de aplica√ß√£o no [Discord Developer Portal](https://discord.com/developers/applications)\n",
    "- Necessita configura√ß√£o de Bot Token e permiss√µes espec√≠ficas\n",
    "- Requer Gateway Intents (Privileged Intents) que precisam ser aprovados manualmente\n",
    "- Tokens sens√≠veis n√£o devem ser expostos em notebooks compartilh√°veis\n",
    "\n",
    "#### 1.2 Requisitos de Infraestrutura Persistente\n",
    "- Discord bots precisam de **conex√£o WebSocket persistente** com os servidores\n",
    "- Notebook executa c√©lulas de forma **n√£o-persistente** (execu√ß√£o pontual)\n",
    "- Desconex√µes frequentes invalidariam o streaming cont√≠nuo\n",
    "- Rate limits do Discord (50 requests/segundo) complicam testes\n",
    "\n",
    "#### 1.3 Depend√™ncias de Servidor/Canal\n",
    "- Necessita servidor Discord espec√≠fico com permiss√µes de administrador\n",
    "- Configura√ß√£o de canais e webhooks externos ao ambiente de teste\n",
    "- N√£o √© reproduz√≠vel sem acesso ao servidor Discord configurado\n",
    "\n",
    "### Alternativa Escolhida: Simulador de Streaming HTTP/RSS\n",
    "\n",
    "Para garantir **reprodutibilidade** e **execu√ß√£o autocontida**, implementamos:\n",
    "\n",
    "1. **Producer Python** que simula mensagens de rede social\n",
    "2. **Fonte de dados**: API p√∫blica de not√≠cias (NewsAPI) ou gera√ß√£o sint√©tica\n",
    "3. **Publica√ß√£o em Kafka** no t√≥pico `social-input`\n",
    "4. **Formato JSON** compat√≠vel com pipeline Spark\n",
    "\n",
    "**Vantagens:**\n",
    "- ‚úÖ Totalmente execut√°vel dentro do notebook\n",
    "- ‚úÖ Sem depend√™ncias de contas externas\n",
    "- ‚úÖ Reproduz√≠vel em qualquer ambiente\n",
    "- ‚úÖ Controle total sobre taxa de mensagens\n",
    "- ‚úÖ Simula caracter√≠sticas de rede social (timestamp, autor, texto)\n",
    "\n",
    "**Refer√™ncias:**\n",
    "- [Discord Developer Docs - Rate Limits](https://discord.com/developers/docs/topics/rate-limits)\n",
    "- [Kafka Documentation - Use Cases](https://kafka.apache.org/documentation/#uses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f881d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Configura√ß√£o do Ambiente\n",
    "\n",
    "### 2.1 Importa√ß√µes e Configura√ß√µes Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b743be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import subprocess\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Configura√ß√µes globais\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"localhost:29092\"\n",
    "KAFKA_TOPIC_INPUT = \"social-input\"\n",
    "KAFKA_TOPIC_OUTPUT = \"wordcount-output\"\n",
    "ELASTICSEARCH_HOST = \"http://localhost:9200\"\n",
    "ELASTICSEARCH_INDEX = \"wordcount-realtime\"\n",
    "SPARK_MASTER = \"spark://localhost:7077\"\n",
    "\n",
    "# Diret√≥rio do projeto\n",
    "PROJECT_DIR = Path(\"/home/edilberto/pspd/atividade-extraclasse-2-pspd/spark\")\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "print(\"‚úì Configura√ß√µes carregadas\")\n",
    "print(f\"  - Kafka: {KAFKA_BOOTSTRAP_SERVERS}\")\n",
    "print(f\"  - T√≥pico entrada: {KAFKA_TOPIC_INPUT}\")\n",
    "print(f\"  - T√≥pico sa√≠da: {KAFKA_TOPIC_OUTPUT}\")\n",
    "print(f\"  - Elasticsearch: {ELASTICSEARCH_HOST}\")\n",
    "print(f\"  - √çndice ES: {ELASTICSEARCH_INDEX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a237b54a",
   "metadata": {},
   "source": [
    "### 2.2 Iniciar Infraestrutura Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f8b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /home/edilberto/pspd/atividade-extraclasse-2-pspd/spark\n",
    "\n",
    "echo \"=== Iniciando containers Docker ===\"\n",
    "docker-compose up -d\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== Aguardando inicializa√ß√£o dos servi√ßos (30s) ===\"\n",
    "sleep 30\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== Status dos containers ===\"\n",
    "docker-compose ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c223fc8",
   "metadata": {},
   "source": [
    "### 2.3 Verificar Sa√∫de dos Servi√ßos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888de67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_service(name, url, timeout=5):\n",
    "    \"\"\"Verifica se um servi√ßo est√° respondendo\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=timeout)\n",
    "        if response.status_code < 400:\n",
    "            print(f\"‚úì {name}: OK (status {response.status_code})\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚úó {name}: ERRO (status {response.status_code})\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó {name}: INACESS√çVEL ({str(e)})\")\n",
    "        return False\n",
    "\n",
    "print(\"=== Verificando Servi√ßos ===\")\n",
    "es_ok = check_service(\"Elasticsearch\", \"http://localhost:9200\")\n",
    "kibana_ok = check_service(\"Kibana\", \"http://localhost:5601\")\n",
    "spark_ok = check_service(\"Spark Master\", \"http://localhost:8080\")\n",
    "\n",
    "if es_ok and kibana_ok and spark_ok:\n",
    "    print(\"\\n‚úì Todos os servi√ßos est√£o operacionais!\")\n",
    "else:\n",
    "    print(\"\\n‚ö† Alguns servi√ßos n√£o responderam. Aguarde mais tempo ou verifique logs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6ce738",
   "metadata": {},
   "source": [
    "### 2.4 Criar T√≥picos Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642fb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"=== Criando t√≥picos Kafka ===\"\n",
    "\n",
    "# Criar t√≥pico de entrada\n",
    "docker exec kafka kafka-topics --create \\\n",
    "  --bootstrap-server localhost:9092 \\\n",
    "  --topic social-input \\\n",
    "  --partitions 3 \\\n",
    "  --replication-factor 1 \\\n",
    "  --if-not-exists\n",
    "\n",
    "# Criar t√≥pico de sa√≠da\n",
    "docker exec kafka kafka-topics --create \\\n",
    "  --bootstrap-server localhost:9092 \\\n",
    "  --topic wordcount-output \\\n",
    "  --partitions 3 \\\n",
    "  --replication-factor 1 \\\n",
    "  --if-not-exists\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== Listando t√≥picos ===\"\n",
    "docker exec kafka kafka-topics --list --bootstrap-server localhost:9092"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954eeedf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Producer Kafka - Simulador de Rede Social\n",
    "\n",
    "### 3.1 Instala√ß√£o de Depend√™ncias Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576c51db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q kafka-python elasticsearch pyspark==3.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e92a96",
   "metadata": {},
   "source": [
    "### 3.2 Implementa√ß√£o do Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d20540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Dataset sint√©tico simulando mensagens de rede social\n",
    "SAMPLE_MESSAGES = [\n",
    "    \"Apache Spark is amazing for big data processing and analytics\",\n",
    "    \"Learning distributed systems with Kafka and Spark\",\n",
    "    \"Real-time data processing using Structured Streaming\",\n",
    "    \"Elasticsearch and Kibana provide powerful visualization tools\",\n",
    "    \"Docker containers make deployment much easier\",\n",
    "    \"Big data analytics requires scalable infrastructure\",\n",
    "    \"Cloud computing enables elastic scalability\",\n",
    "    \"Machine learning models benefit from distributed training\",\n",
    "    \"Parallel computing accelerates data processing pipelines\",\n",
    "    \"Kafka provides reliable message streaming capabilities\",\n",
    "    \"Python is a versatile language for data science\",\n",
    "    \"Distributed databases handle massive datasets efficiently\",\n",
    "    \"Stream processing enables real-time analytics\",\n",
    "    \"Microservices architecture improves system modularity\",\n",
    "    \"Data pipelines transform raw data into insights\"\n",
    "]\n",
    "\n",
    "SAMPLE_USERS = [\"alice\", \"bob\", \"charlie\", \"diana\", \"eve\", \"frank\", \"grace\", \"henry\"]\n",
    "\n",
    "class SocialMediaProducer:\n",
    "    \"\"\"Producer que simula mensagens de rede social\"\"\"\n",
    "    \n",
    "    def __init__(self, bootstrap_servers, topic):\n",
    "        self.producer = KafkaProducer(\n",
    "            bootstrap_servers=bootstrap_servers,\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "        )\n",
    "        self.topic = topic\n",
    "        self.running = False\n",
    "        self.message_count = 0\n",
    "        \n",
    "    def generate_message(self):\n",
    "        \"\"\"Gera uma mensagem sint√©tica\"\"\"\n",
    "        return {\n",
    "            \"user\": random.choice(SAMPLE_USERS),\n",
    "            \"text\": random.choice(SAMPLE_MESSAGES),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"platform\": \"SimulatedSocial\",\n",
    "            \"message_id\": self.message_count\n",
    "        }\n",
    "    \n",
    "    def start(self, duration_seconds=300, messages_per_second=2):\n",
    "        \"\"\"Inicia o producer por um per√≠odo determinado\"\"\"\n",
    "        self.running = True\n",
    "        start_time = time.time()\n",
    "        interval = 1.0 / messages_per_second\n",
    "        \n",
    "        print(f\"‚ñ∂ Producer iniciado: {messages_per_second} msgs/seg por {duration_seconds}s\")\n",
    "        \n",
    "        try:\n",
    "            while self.running and (time.time() - start_time) < duration_seconds:\n",
    "                message = self.generate_message()\n",
    "                self.producer.send(self.topic, value=message)\n",
    "                self.message_count += 1\n",
    "                \n",
    "                if self.message_count % 10 == 0:\n",
    "                    print(f\"  [{self.message_count}] Enviado: {message['user']}: {message['text'][:50]}...\")\n",
    "                \n",
    "                time.sleep(interval)\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n‚è∏ Producer interrompido pelo usu√°rio\")\n",
    "        finally:\n",
    "            self.stop()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Para o producer\"\"\"\n",
    "        self.running = False\n",
    "        self.producer.flush()\n",
    "        self.producer.close()\n",
    "        print(f\"‚ñ† Producer parado. Total de mensagens: {self.message_count}\")\n",
    "\n",
    "# Criar producer (ser√° iniciado posteriormente)\n",
    "producer = SocialMediaProducer(KAFKA_BOOTSTRAP_SERVERS, KAFKA_TOPIC_INPUT)\n",
    "print(\"‚úì Producer configurado e pronto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de523908",
   "metadata": {},
   "source": [
    "### 3.3 Testar Producer (Envio de Amostra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742ca46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enviar 20 mensagens de teste\n",
    "print(\"=== Teste do Producer (20 mensagens) ===\")\n",
    "test_producer = SocialMediaProducer(KAFKA_BOOTSTRAP_SERVERS, KAFKA_TOPIC_INPUT)\n",
    "test_producer.start(duration_seconds=10, messages_per_second=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23164427",
   "metadata": {},
   "source": [
    "### 3.4 Verificar Mensagens no T√≥pico Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffc9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"=== √öltimas 5 mensagens no t√≥pico social-input ===\"\n",
    "docker exec kafka kafka-console-consumer \\\n",
    "  --bootstrap-server localhost:9092 \\\n",
    "  --topic social-input \\\n",
    "  --from-beginning \\\n",
    "  --max-messages 5 \\\n",
    "  --timeout-ms 5000 2>/dev/null | tail -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54408fd9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Pipeline Spark Structured Streaming\n",
    "\n",
    "### 4.1 Configura√ß√£o da Sess√£o Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d5821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, explode, split, window, from_json, to_json, struct, current_timestamp\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# Criar sess√£o Spark com suporte a Kafka\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"B2_SocialMedia_WordCount\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úì Sess√£o Spark criada\")\n",
    "print(f\"  - Vers√£o: {spark.version}\")\n",
    "print(f\"  - App: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fad717",
   "metadata": {},
   "source": [
    "### 4.2 Schema das Mensagens de Entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema JSON das mensagens do producer\n",
    "message_schema = StructType([\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"platform\", StringType(), True),\n",
    "    StructField(\"message_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "print(\"‚úì Schema definido:\")\n",
    "message_schema.printTreeString()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2660b",
   "metadata": {},
   "source": [
    "### 4.3 Ler Stream do Kafka (Entrada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce34541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler mensagens do t√≥pico de entrada\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC_INPUT) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON e extrair campos\n",
    "messages = raw_stream.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), message_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"event_time\", current_timestamp())\n",
    "\n",
    "print(\"‚úì Stream de entrada configurado\")\n",
    "print(\"  Schema do stream:\")\n",
    "messages.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25b05fd",
   "metadata": {},
   "source": [
    "### 4.4 Processamento: Word Count com Janelas Temporais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbbc964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair palavras do texto\n",
    "words = messages.select(\n",
    "    explode(split(col(\"text\"), \"\\\\s+\")).alias(\"word\"),\n",
    "    col(\"event_time\"),\n",
    "    col(\"user\")\n",
    ").filter(col(\"word\") != \"\")\n",
    "\n",
    "# Normalizar palavras (lowercase, remover pontua√ß√£o)\n",
    "from pyspark.sql.functions import lower, regexp_replace\n",
    "\n",
    "words_clean = words.withColumn(\n",
    "    \"word\", \n",
    "    lower(regexp_replace(col(\"word\"), \"[^a-zA-Z0-9]\", \"\"))\n",
    ").filter(col(\"word\") != \"\")\n",
    "\n",
    "# Agrega√ß√£o com janelas de 30 segundos\n",
    "WINDOW_DURATION = \"30 seconds\"\n",
    "SLIDE_DURATION = \"10 seconds\"\n",
    "\n",
    "word_counts = words_clean \\\n",
    "    .withWatermark(\"event_time\", \"1 minute\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), WINDOW_DURATION, SLIDE_DURATION),\n",
    "        col(\"word\")\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"word\"),\n",
    "        col(\"count\")\n",
    "    )\n",
    "\n",
    "print(\"‚úì Pipeline de word count configurado\")\n",
    "print(f\"  - Janela: {WINDOW_DURATION}\")\n",
    "print(f\"  - Slide: {SLIDE_DURATION}\")\n",
    "print(f\"  - Watermark: 1 minuto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aed397",
   "metadata": {},
   "source": [
    "### 4.5 Escrever Resultados no Kafka (Sa√≠da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b220b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter para JSON\n",
    "output_json = word_counts.select(\n",
    "    to_json(struct(\n",
    "        col(\"word\"),\n",
    "        col(\"count\"),\n",
    "        col(\"window_start\"),\n",
    "        col(\"window_end\")\n",
    "    )).alias(\"value\")\n",
    ")\n",
    "\n",
    "# Escrever no t√≥pico de sa√≠da\n",
    "query_kafka = output_json.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"topic\", KAFKA_TOPIC_OUTPUT) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/spark-checkpoint-kafka\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"‚úì Stream de sa√≠da para Kafka iniciado\")\n",
    "print(f\"  - Query ID: {query_kafka.id}\")\n",
    "print(f\"  - T√≥pico: {KAFKA_TOPIC_OUTPUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e942461",
   "metadata": {},
   "source": [
    "### 4.6 Visualiza√ß√£o em Console (Debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dac095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query adicional para visualizar resultados no console\n",
    "query_console = word_counts.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 20) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"‚úì Stream de debug (console) iniciado\")\n",
    "print(f\"  - Query ID: {query_console.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcdb6b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Iniciar Producer em Background\n",
    "\n",
    "### 5.1 Executar Producer em Thread Separada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff3cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Configura√ß√£o do producer\n",
    "PRODUCER_DURATION = 180  # 3 minutos\n",
    "MESSAGES_PER_SECOND = 3\n",
    "\n",
    "# Criar novo producer\n",
    "background_producer = SocialMediaProducer(KAFKA_BOOTSTRAP_SERVERS, KAFKA_TOPIC_INPUT)\n",
    "\n",
    "# Fun√ß√£o para executar em thread\n",
    "def run_producer():\n",
    "    background_producer.start(\n",
    "        duration_seconds=PRODUCER_DURATION,\n",
    "        messages_per_second=MESSAGES_PER_SECOND\n",
    "    )\n",
    "\n",
    "# Iniciar thread\n",
    "producer_thread = threading.Thread(target=run_producer, daemon=True)\n",
    "producer_thread.start()\n",
    "\n",
    "print(f\"‚úì Producer iniciado em background\")\n",
    "print(f\"  - Dura√ß√£o: {PRODUCER_DURATION}s\")\n",
    "print(f\"  - Taxa: {MESSAGES_PER_SECOND} msgs/seg\")\n",
    "print(f\"  - Total esperado: ~{PRODUCER_DURATION * MESSAGES_PER_SECOND} mensagens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacbe351",
   "metadata": {},
   "source": [
    "### 5.2 Monitorar Queries Spark (Aguardar Processamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e74d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"=== Monitorando Queries Spark ===\")\n",
    "print(\"Aguarde 60 segundos para acumular dados...\\n\")\n",
    "\n",
    "for i in range(6):\n",
    "    time.sleep(10)\n",
    "    print(f\"[{(i+1)*10}s] Query Kafka: {query_kafka.status['message']}\")\n",
    "    print(f\"[{(i+1)*10}s] Query Console: {query_console.status['message']}\")\n",
    "    \n",
    "    # Mostrar progresso\n",
    "    if 'numInputRows' in query_kafka.lastProgress:\n",
    "        input_rows = query_kafka.lastProgress['numInputRows']\n",
    "        print(f\"       ‚Üí Linhas processadas no √∫ltimo batch: {input_rows}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úì Processamento em andamento. Verifique console acima para word counts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35152595",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Consumer Elasticsearch\n",
    "\n",
    "### 6.1 Criar √çndice no Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54db31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "# Conectar ao Elasticsearch\n",
    "es = Elasticsearch([ELASTICSEARCH_HOST])\n",
    "\n",
    "# Verificar conex√£o\n",
    "if es.ping():\n",
    "    print(\"‚úì Conectado ao Elasticsearch\")\n",
    "    print(f\"  - Cluster: {es.info()['cluster_name']}\")\n",
    "    print(f\"  - Vers√£o: {es.info()['version']['number']}\")\n",
    "else:\n",
    "    print(\"‚úó Falha ao conectar no Elasticsearch\")\n",
    "\n",
    "# Criar √≠ndice com mapping\n",
    "index_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"word\": {\"type\": \"keyword\"},\n",
    "            \"count\": {\"type\": \"integer\"},\n",
    "            \"window_start\": {\"type\": \"date\"},\n",
    "            \"window_end\": {\"type\": \"date\"},\n",
    "            \"indexed_at\": {\"type\": \"date\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Deletar √≠ndice existente (se houver)\n",
    "if es.indices.exists(index=ELASTICSEARCH_INDEX):\n",
    "    es.indices.delete(index=ELASTICSEARCH_INDEX)\n",
    "    print(f\"  - √çndice '{ELASTICSEARCH_INDEX}' deletado\")\n",
    "\n",
    "# Criar novo √≠ndice\n",
    "es.indices.create(index=ELASTICSEARCH_INDEX, body=index_mapping)\n",
    "print(f\"‚úì √çndice '{ELASTICSEARCH_INDEX}' criado com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e54435f",
   "metadata": {},
   "source": [
    "### 6.2 Consumer Kafka ‚Üí Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a45fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class ElasticsearchConsumer:\n",
    "    \"\"\"Consumer que indexa word counts no Elasticsearch\"\"\"\n",
    "    \n",
    "    def __init__(self, bootstrap_servers, topic, es_client, es_index):\n",
    "        self.consumer = KafkaConsumer(\n",
    "            topic,\n",
    "            bootstrap_servers=bootstrap_servers,\n",
    "            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "            auto_offset_reset='latest',\n",
    "            enable_auto_commit=True,\n",
    "            group_id='elasticsearch-consumer-group'\n",
    "        )\n",
    "        self.es = es_client\n",
    "        self.index = es_index\n",
    "        self.running = False\n",
    "        self.document_count = 0\n",
    "        \n",
    "    def start(self, duration_seconds=120, batch_size=50):\n",
    "        \"\"\"Inicia o consumer por um per√≠odo determinado\"\"\"\n",
    "        self.running = True\n",
    "        start_time = time.time()\n",
    "        batch = []\n",
    "        \n",
    "        print(f\"‚ñ∂ Consumer Elasticsearch iniciado ({duration_seconds}s)\")\n",
    "        \n",
    "        try:\n",
    "            for message in self.consumer:\n",
    "                if (time.time() - start_time) > duration_seconds:\n",
    "                    break\n",
    "                \n",
    "                data = message.value\n",
    "                \n",
    "                # Preparar documento\n",
    "                doc = {\n",
    "                    \"_index\": self.index,\n",
    "                    \"_source\": {\n",
    "                        \"word\": data.get(\"word\"),\n",
    "                        \"count\": data.get(\"count\"),\n",
    "                        \"window_start\": data.get(\"window_start\"),\n",
    "                        \"window_end\": data.get(\"window_end\"),\n",
    "                        \"indexed_at\": datetime.now().isoformat()\n",
    "                    }\n",
    "                }\n",
    "                batch.append(doc)\n",
    "                self.document_count += 1\n",
    "                \n",
    "                # Indexar em lote\n",
    "                if len(batch) >= batch_size:\n",
    "                    success, _ = bulk(self.es, batch)\n",
    "                    print(f\"  [{self.document_count}] Indexados {success} documentos\")\n",
    "                    batch = []\n",
    "            \n",
    "            # Indexar documentos restantes\n",
    "            if batch:\n",
    "                success, _ = bulk(self.es, batch)\n",
    "                print(f\"  [Final] Indexados {success} documentos\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n‚è∏ Consumer interrompido pelo usu√°rio\")\n",
    "        finally:\n",
    "            self.stop()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Para o consumer\"\"\"\n",
    "        self.running = False\n",
    "        self.consumer.close()\n",
    "        print(f\"‚ñ† Consumer parado. Total de documentos: {self.document_count}\")\n",
    "\n",
    "# Criar consumer\n",
    "es_consumer = ElasticsearchConsumer(\n",
    "    KAFKA_BOOTSTRAP_SERVERS,\n",
    "    KAFKA_TOPIC_OUTPUT,\n",
    "    es,\n",
    "    ELASTICSEARCH_INDEX\n",
    ")\n",
    "\n",
    "print(\"‚úì Consumer Elasticsearch configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d9b74",
   "metadata": {},
   "source": [
    "### 6.3 Executar Consumer em Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_DURATION = 120  # 2 minutos\n",
    "\n",
    "def run_consumer():\n",
    "    es_consumer.start(duration_seconds=CONSUMER_DURATION, batch_size=30)\n",
    "\n",
    "# Iniciar thread do consumer\n",
    "consumer_thread = threading.Thread(target=run_consumer, daemon=True)\n",
    "consumer_thread.start()\n",
    "\n",
    "print(f\"‚úì Consumer iniciado em background\")\n",
    "print(f\"  - Dura√ß√£o: {CONSUMER_DURATION}s\")\n",
    "print(f\"  - Batch size: 30 documentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d35505",
   "metadata": {},
   "source": [
    "### 6.4 Aguardar e Verificar Indexa√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc8651",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Aguardando indexa√ß√£o (90 segundos) ===\")\n",
    "time.sleep(90)\n",
    "\n",
    "# Verificar contagem de documentos\n",
    "es.indices.refresh(index=ELASTICSEARCH_INDEX)\n",
    "count = es.count(index=ELASTICSEARCH_INDEX)['count']\n",
    "\n",
    "print(f\"\\n‚úì Documentos indexados: {count}\")\n",
    "\n",
    "# Mostrar amostra\n",
    "if count > 0:\n",
    "    print(\"\\n=== Amostra de 10 word counts ===\")\n",
    "    result = es.search(\n",
    "        index=ELASTICSEARCH_INDEX,\n",
    "        body={\n",
    "            \"size\": 10,\n",
    "            \"sort\": [{\"count\": {\"order\": \"desc\"}}]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for hit in result['hits']['hits']:\n",
    "        doc = hit['_source']\n",
    "        print(f\"  {doc['word']:20s} ‚Üí {doc['count']:3d} ocorr√™ncias\")\n",
    "else:\n",
    "    print(\"‚ö† Nenhum documento indexado ainda. Aguarde mais tempo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ee2e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Visualiza√ß√£o no Kibana\n",
    "\n",
    "### 7.1 Instru√ß√µes para Criar Nuvem de Palavras\n",
    "\n",
    "#### Passo 1: Acessar Kibana\n",
    "1. Abra o navegador em: **http://localhost:5601**\n",
    "2. Aguarde o Kibana carregar (pode levar alguns segundos)\n",
    "\n",
    "#### Passo 2: Criar Index Pattern\n",
    "1. No menu lateral, clique em **\"Stack Management\"** (√≠cone de engrenagem)\n",
    "2. Em **\"Kibana\"**, clique em **\"Data Views\"** (ou **\"Index Patterns\"**)\n",
    "3. Clique em **\"Create data view\"**\n",
    "4. Configure:\n",
    "   - **Name:** `WordCount Real-Time`\n",
    "   - **Index pattern:** `wordcount-realtime*`\n",
    "   - **Timestamp field:** `window_start`\n",
    "5. Clique em **\"Create data view\"**\n",
    "\n",
    "#### Passo 3: Criar Visualiza√ß√£o Tag Cloud\n",
    "1. No menu lateral, clique em **\"Visualize Library\"** (√≠cone de gr√°fico)\n",
    "2. Clique em **\"Create visualization\"**\n",
    "3. Selecione o tipo **\"Tag Cloud\"** (nuvem de tags)\n",
    "4. Selecione o data view **\"WordCount Real-Time\"**\n",
    "5. Configure a visualiza√ß√£o:\n",
    "   - **Buckets ‚Üí Tags:**\n",
    "     - Aggregation: `Terms`\n",
    "     - Field: `word.keyword`\n",
    "     - Order By: `Metric: Count`\n",
    "     - Order: `Descending`\n",
    "     - Size: `50` (top 50 palavras)\n",
    "   - **Metrics:**\n",
    "     - Aggregation: `Sum`\n",
    "     - Field: `count`\n",
    "6. Clique em **\"Update\"** (‚ñ∂Ô∏è) para aplicar\n",
    "7. Ajuste o time range no canto superior direito (ex: \"Last 15 minutes\")\n",
    "8. Salve a visualiza√ß√£o:\n",
    "   - Clique em **\"Save\"** no topo\n",
    "   - Nome: `Word Cloud - Social Media Stream`\n",
    "\n",
    "#### Passo 4: Criar Dashboard\n",
    "1. No menu lateral, clique em **\"Dashboard\"**\n",
    "2. Clique em **\"Create dashboard\"**\n",
    "3. Clique em **\"Add\"** ‚Üí Selecione `Word Cloud - Social Media Stream`\n",
    "4. Adicione visualiza√ß√µes complementares:\n",
    "   - **Vertical Bar**: Count por janela temporal\n",
    "   - **Data Table**: Top 20 palavras\n",
    "   - **Metric**: Total de palavras √∫nicas\n",
    "5. Configure auto-refresh:\n",
    "   - Clique no rel√≥gio no topo\n",
    "   - Selecione **\"Auto-refresh\"** ‚Üí `10 seconds`\n",
    "6. Salve o dashboard:\n",
    "   - Nome: `B2 - Real-Time Word Count Analytics`\n",
    "\n",
    "### 7.2 Alternativas ao Tag Cloud\n",
    "\n",
    "Se o Tag Cloud n√£o estiver dispon√≠vel na sua vers√£o do Kibana:\n",
    "\n",
    "**Op√ß√£o A: Horizontal Bar Chart**\n",
    "- Tipo: `Horizontal Bar`\n",
    "- Y-axis: `word.keyword` (Terms, top 30)\n",
    "- X-axis: `count` (Sum)\n",
    "- Visualiza as palavras mais frequentes em barras horizontais\n",
    "\n",
    "**Op√ß√£o B: Data Table**\n",
    "- Tipo: `Data Table`\n",
    "- Rows: `word.keyword` (Terms, top 50)\n",
    "- Metrics: `count` (Sum)\n",
    "- Visualiza tabela ordenada por contagem\n",
    "\n",
    "**Op√ß√£o C: Treemap**\n",
    "- Tipo: `Treemap`\n",
    "- Groups: `word.keyword` (Terms, top 40)\n",
    "- Size: `count` (Sum)\n",
    "- Visualiza palavras em blocos proporcionais √† frequ√™ncia\n",
    "\n",
    "### 7.3 Screenshot do Dashboard\n",
    "\n",
    "**A√á√ÉO MANUAL REQUERIDA:**\n",
    "1. Ap√≥s criar o dashboard, tire um screenshot completo\n",
    "2. Salve como: `resultados_spark/kibana_dashboard_wordcloud.png`\n",
    "3. Capture tamb√©m a visualiza√ß√£o Tag Cloud isolada\n",
    "4. Salve como: `resultados_spark/kibana_tagcloud_detail.png`\n",
    "\n",
    "### 7.4 Verificar Dados no Kibana via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10055040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se o √≠ndice est√° vis√≠vel no Kibana\n",
    "import requests\n",
    "\n",
    "kibana_url = \"http://localhost:5601\"\n",
    "\n",
    "print(\"=== Verifica√ß√£o Kibana ===\")\n",
    "try:\n",
    "    # Check Kibana status\n",
    "    response = requests.get(f\"{kibana_url}/api/status\")\n",
    "    if response.status_code == 200:\n",
    "        print(\"‚úì Kibana est√° acess√≠vel\")\n",
    "        print(f\"  URL: {kibana_url}\")\n",
    "        print(f\"\\nüìä Acesse o Kibana e siga as instru√ß√µes acima para criar a visualiza√ß√£o\")\n",
    "    else:\n",
    "        print(f\"‚ö† Kibana retornou status {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Erro ao conectar no Kibana: {e}\")\n",
    "\n",
    "# Mostrar estat√≠sticas do √≠ndice\n",
    "print(f\"\\n=== Estat√≠sticas do √çndice '{ELASTICSEARCH_INDEX}' ===\")\n",
    "stats = es.indices.stats(index=ELASTICSEARCH_INDEX)\n",
    "print(f\"  - Total de documentos: {stats['_all']['total']['docs']['count']}\")\n",
    "print(f\"  - Tamanho em disco: {stats['_all']['total']['store']['size_in_bytes'] / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d235497f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Parar Streams e Limpar Recursos\n",
    "\n",
    "### 8.1 Parar Queries Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b639b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Parando Queries Spark ===\")\n",
    "\n",
    "# Parar query Kafka\n",
    "if query_kafka.isActive:\n",
    "    query_kafka.stop()\n",
    "    print(\"‚úì Query Kafka parada\")\n",
    "\n",
    "# Parar query Console\n",
    "if query_console.isActive:\n",
    "    query_console.stop()\n",
    "    print(\"‚úì Query Console parada\")\n",
    "\n",
    "# Aguardar threads terminarem\n",
    "time.sleep(5)\n",
    "\n",
    "print(\"\\n‚úì Todas as queries foram paradas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d8bd87",
   "metadata": {},
   "source": [
    "### 8.2 Fechar Sess√£o Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d212d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"‚úì Sess√£o Spark encerrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b354a",
   "metadata": {},
   "source": [
    "### 8.3 Estat√≠sticas Finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937b09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ESTAT√çSTICAS FINAIS ===\")\n",
    "\n",
    "# Contagem final no Elasticsearch\n",
    "es.indices.refresh(index=ELASTICSEARCH_INDEX)\n",
    "final_count = es.count(index=ELASTICSEARCH_INDEX)['count']\n",
    "\n",
    "print(f\"\\nüìä Documentos indexados no Elasticsearch: {final_count}\")\n",
    "\n",
    "# Top 20 palavras\n",
    "if final_count > 0:\n",
    "    print(\"\\nüìà Top 20 Palavras Mais Frequentes:\")\n",
    "    result = es.search(\n",
    "        index=ELASTICSEARCH_INDEX,\n",
    "        body={\n",
    "            \"size\": 0,\n",
    "            \"aggs\": {\n",
    "                \"top_words\": {\n",
    "                    \"terms\": {\n",
    "                        \"field\": \"word.keyword\",\n",
    "                        \"size\": 20,\n",
    "                        \"order\": {\"total_count\": \"desc\"}\n",
    "                    },\n",
    "                    \"aggs\": {\n",
    "                        \"total_count\": {\n",
    "                            \"sum\": {\"field\": \"count\"}\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    for i, bucket in enumerate(result['aggregations']['top_words']['buckets'], 1):\n",
    "        word = bucket['key']\n",
    "        count = int(bucket['total_count']['value'])\n",
    "        print(f\"  {i:2d}. {word:20s} ‚Üí {count:4d} ocorr√™ncias\")\n",
    "\n",
    "# Verificar t√≥picos Kafka\n",
    "print(\"\\nüì® T√≥picos Kafka:\")\n",
    "!docker exec kafka kafka-topics --list --bootstrap-server localhost:9092\n",
    "\n",
    "print(\"\\n‚úì Pipeline B2 executado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b483d10",
   "metadata": {},
   "source": [
    "### 8.4.7 Visualiza√ß√£o de Sentimentos no Kibana (OPCIONAL)\n",
    "\n",
    "**Ap√≥s executar o pipeline de sentimentos**, criar visualiza√ß√µes no Kibana:\n",
    "\n",
    "#### 1. Criar Data View para Sentimentos\n",
    "- Stack Management ‚Üí Data Views\n",
    "- Name: `Social Sentiment Analysis`\n",
    "- Index pattern: `social-sentiment*`\n",
    "- Timestamp: `timestamp`\n",
    "\n",
    "#### 2. Visualiza√ß√£o: Pie Chart - Distribui√ß√£o de Sentimentos\n",
    "- Tipo: **Pie Chart**\n",
    "- Metric: Count\n",
    "- Buckets: Terms by `sentiment_classification.keyword`\n",
    "- Mostra propor√ß√£o de mensagens positivas/neutras/negativas\n",
    "\n",
    "#### 3. Visualiza√ß√£o: Line Chart - Sentimento ao Longo do Tempo\n",
    "- Tipo: **Line**\n",
    "- X-axis: Date Histogram on `timestamp`\n",
    "- Y-axis: Average of `sentiment_compound`\n",
    "- Split series: Terms by `sentiment_classification.keyword`\n",
    "- Mostra evolu√ß√£o temporal dos sentimentos\n",
    "\n",
    "#### 4. Visualiza√ß√£o: Data Table - Top Mensagens por Sentimento\n",
    "- Tipo: **Data Table**\n",
    "- Rows: Terms by `sentiment_classification.keyword`\n",
    "- Metrics: \n",
    "  - Count\n",
    "  - Average of `sentiment_compound`\n",
    "  - Min/Max of `sentiment_compound`\n",
    "\n",
    "#### 5. Dashboard Completo de Sentimentos\n",
    "Combinar:\n",
    "- Pie Chart (distribui√ß√£o)\n",
    "- Line Chart (temporal)\n",
    "- Data Table (estat√≠sticas)\n",
    "- Metric: Total de mensagens analisadas\n",
    "- Filter: Por classifica√ß√£o de sentimento\n",
    "\n",
    "### Refer√™ncias Adicionais para An√°lise de Sentimentos\n",
    "\n",
    "1. **VADER Original:**\n",
    "   - Hutto & Gilbert (2014) - [Paper ICWSM](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf)\n",
    "\n",
    "2. **Implementa√ß√£o Python:**\n",
    "   - [vaderSentiment GitHub](https://github.com/cjhutto/vaderSentiment)\n",
    "   - [Documentation](https://github.com/cjhutto/vaderSentiment#about-the-scoring)\n",
    "\n",
    "3. **Alternativas:**\n",
    "   - TextBlob: Mais simples, menos preciso para redes sociais\n",
    "   - BERT/Transformers: Mais preciso, requer GPU e treinamento\n",
    "   - NLTK: Requer corpus e mais configura√ß√£o\n",
    "\n",
    "4. **Aplica√ß√µes em Streaming:**\n",
    "   - Liu, B. (2015). Sentiment Analysis: Mining Opinions, Sentiments, and Emotions. Cambridge University Press.\n",
    "   - Medhat, W., Hassan, A., & Korashy, H. (2014). Sentiment analysis algorithms and applications: A survey. Ain Shams Engineering Journal.\n",
    "\n",
    "**Diferencial deste Trabalho:**\n",
    "- Integra√ß√£o nativa com Kafka Streaming (n√£o batch)\n",
    "- Indexa√ß√£o em tempo real no Elasticsearch\n",
    "- Visualiza√ß√£o din√¢mica no Kibana\n",
    "- Pipeline 100% em notebook (reproduz√≠vel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f15f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPCIONAL: Executar consumer com an√°lise de sentimentos\n",
    "# Descomentar para executar\n",
    "\n",
    "# sentiment_consumer = SentimentElasticsearchConsumer(\n",
    "#     KAFKA_BOOTSTRAP_SERVERS,\n",
    "#     \"sentiment-input\",\n",
    "#     es,\n",
    "#     SENTIMENT_INDEX\n",
    "# )\n",
    "\n",
    "# def run_sentiment_consumer():\n",
    "#     sentiment_consumer.start(duration_seconds=120, batch_size=20)\n",
    "\n",
    "# consumer_sentiment_thread = threading.Thread(target=run_sentiment_consumer, daemon=True)\n",
    "# consumer_sentiment_thread.start()\n",
    "\n",
    "# print(\"‚úì Consumer de sentimentos iniciado (120s, batch=20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ecfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPCIONAL: Executar producer com sentimentos\n",
    "# Descomentar para executar\n",
    "\n",
    "# sentiment_producer = SentimentProducer(KAFKA_BOOTSTRAP_SERVERS, \"sentiment-input\")\n",
    "\n",
    "# def run_sentiment_producer():\n",
    "#     sentiment_producer.start(duration_seconds=120, messages_per_second=2)\n",
    "\n",
    "# sentiment_thread = threading.Thread(target=run_sentiment_producer, daemon=True)\n",
    "# sentiment_thread.start()\n",
    "\n",
    "# print(\"‚úì Producer de sentimentos iniciado (120s, 2 msgs/seg)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPCIONAL: Criar t√≥pico para sentimentos\n",
    "# Descomentar para executar\n",
    "\n",
    "# !docker exec kafka kafka-topics --create \\\n",
    "#   --bootstrap-server localhost:9092 \\\n",
    "#   --topic sentiment-input \\\n",
    "#   --partitions 3 \\\n",
    "#   --replication-factor 1 \\\n",
    "#   --if-not-exists\n",
    "\n",
    "# print(\"‚úì T√≥pico 'sentiment-input' criado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f9232",
   "metadata": {},
   "source": [
    "### 8.4.6 Executar Pipeline com An√°lise de Sentimentos (OPCIONAL)\n",
    "\n",
    "**INSTRU√á√ïES:**\n",
    "1. Descomentar as c√©lulas abaixo para executar\n",
    "2. Ou executar diretamente se quiser testar an√°lise de sentimentos\n",
    "3. Criar novo t√≥pico Kafka `sentiment-input`\n",
    "4. Iniciar producer com mensagens variadas\n",
    "5. Iniciar consumer com an√°lise VADER\n",
    "\n",
    "**NOTA:** Esta √© uma extens√£o opcional. O pipeline principal (Se√ß√µes 1-8.3) j√° est√° completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6f7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# √çndice para mensagens com an√°lise de sentimentos\n",
    "SENTIMENT_INDEX = \"social-sentiment\"\n",
    "\n",
    "# Mapping otimizado\n",
    "sentiment_mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"user\": {\"type\": \"keyword\"},\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"timestamp\": {\"type\": \"date\"},\n",
    "            \"platform\": {\"type\": \"keyword\"},\n",
    "            \"sentiment_classification\": {\"type\": \"keyword\"},\n",
    "            \"sentiment_compound\": {\"type\": \"float\"},\n",
    "            \"sentiment_pos\": {\"type\": \"float\"},\n",
    "            \"sentiment_neu\": {\"type\": \"float\"},\n",
    "            \"sentiment_neg\": {\"type\": \"float\"},\n",
    "            \"indexed_at\": {\"type\": \"date\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Deletar √≠ndice existente\n",
    "if es.indices.exists(index=SENTIMENT_INDEX):\n",
    "    es.indices.delete(index=SENTIMENT_INDEX)\n",
    "    print(f\"  - √çndice '{SENTIMENT_INDEX}' deletado\")\n",
    "\n",
    "# Criar √≠ndice\n",
    "es.indices.create(index=SENTIMENT_INDEX, body=sentiment_mapping)\n",
    "print(f\"‚úì √çndice '{SENTIMENT_INDEX}' criado\")\n",
    "print(\"  - Campos: user, text, timestamp, platform\")\n",
    "print(\"  - Sentimentos: classification, compound, pos, neu, neg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ac304",
   "metadata": {},
   "source": [
    "### 8.4.5 Criar √çndice Elasticsearch para Sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e52bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentElasticsearchConsumer:\n",
    "    \"\"\"Consumer que analisa sentimentos e indexa no Elasticsearch\"\"\"\n",
    "    \n",
    "    def __init__(self, bootstrap_servers, topic, es_client, es_index):\n",
    "        self.consumer = KafkaConsumer(\n",
    "            topic,\n",
    "            bootstrap_servers=bootstrap_servers,\n",
    "            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "            auto_offset_reset='latest',\n",
    "            enable_auto_commit=True,\n",
    "            group_id='sentiment-consumer-group'\n",
    "        )\n",
    "        self.es = es_client\n",
    "        self.index = es_index\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "        self.running = False\n",
    "        self.document_count = 0\n",
    "        self.sentiment_stats = {\"positive\": 0, \"neutral\": 0, \"negative\": 0}\n",
    "        \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"Analisa sentimento do texto\"\"\"\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        \n",
    "        # Classificar\n",
    "        if scores['compound'] >= 0.05:\n",
    "            classification = \"positive\"\n",
    "        elif scores['compound'] <= -0.05:\n",
    "            classification = \"negative\"\n",
    "        else:\n",
    "            classification = \"neutral\"\n",
    "        \n",
    "        return {\n",
    "            \"classification\": classification,\n",
    "            \"compound\": scores['compound'],\n",
    "            \"positive\": scores['pos'],\n",
    "            \"neutral\": scores['neu'],\n",
    "            \"negative\": scores['neg']\n",
    "        }\n",
    "    \n",
    "    def start(self, duration_seconds=120, batch_size=30):\n",
    "        \"\"\"Inicia consumer com an√°lise de sentimentos\"\"\"\n",
    "        self.running = True\n",
    "        start_time = time.time()\n",
    "        batch = []\n",
    "        \n",
    "        print(f\"‚ñ∂ Consumer Sentiment+ES iniciado ({duration_seconds}s)\")\n",
    "        \n",
    "        try:\n",
    "            for message in self.consumer:\n",
    "                if (time.time() - start_time) > duration_seconds:\n",
    "                    break\n",
    "                \n",
    "                data = message.value\n",
    "                text = data.get(\"text\", \"\")\n",
    "                \n",
    "                # Analisar sentimento\n",
    "                sentiment = self.analyze_sentiment(text)\n",
    "                \n",
    "                # Preparar documento\n",
    "                doc = {\n",
    "                    \"_index\": self.index,\n",
    "                    \"_source\": {\n",
    "                        \"user\": data.get(\"user\"),\n",
    "                        \"text\": text,\n",
    "                        \"timestamp\": data.get(\"timestamp\"),\n",
    "                        \"platform\": data.get(\"platform\"),\n",
    "                        \"sentiment_classification\": sentiment[\"classification\"],\n",
    "                        \"sentiment_compound\": sentiment[\"compound\"],\n",
    "                        \"sentiment_pos\": sentiment[\"positive\"],\n",
    "                        \"sentiment_neu\": sentiment[\"neutral\"],\n",
    "                        \"sentiment_neg\": sentiment[\"negative\"],\n",
    "                        \"indexed_at\": datetime.now().isoformat()\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                batch.append(doc)\n",
    "                self.document_count += 1\n",
    "                self.sentiment_stats[sentiment[\"classification\"]] += 1\n",
    "                \n",
    "                # Indexar em lote\n",
    "                if len(batch) >= batch_size:\n",
    "                    success, _ = bulk(self.es, batch)\n",
    "                    print(f\"  [{self.document_count}] Indexados {success} docs | \"\n",
    "                          f\"Pos: {self.sentiment_stats['positive']} | \"\n",
    "                          f\"Neu: {self.sentiment_stats['neutral']} | \"\n",
    "                          f\"Neg: {self.sentiment_stats['negative']}\")\n",
    "                    batch = []\n",
    "            \n",
    "            # Indexar documentos restantes\n",
    "            if batch:\n",
    "                success, _ = bulk(self.es, batch)\n",
    "                print(f\"  [Final] Indexados {success} documentos\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n‚è∏ Consumer interrompido\")\n",
    "        finally:\n",
    "            self.stop()\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Para consumer\"\"\"\n",
    "        self.running = False\n",
    "        self.consumer.close()\n",
    "        print(f\"‚ñ† Consumer parado. Total: {self.document_count} docs\")\n",
    "        print(f\"  Distribui√ß√£o: Positivo={self.sentiment_stats['positive']}, \"\n",
    "              f\"Neutro={self.sentiment_stats['neutral']}, \"\n",
    "              f\"Negativo={self.sentiment_stats['negative']}\")\n",
    "\n",
    "print(\"‚úì Consumer com an√°lise de sentimentos criado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83bb3a2",
   "metadata": {},
   "source": [
    "### 8.4.4 Consumer com An√°lise de Sentimentos + Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eef615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset expandido com mensagens de sentimento variado\n",
    "SENTIMENT_MESSAGES = [\n",
    "    # Positivas\n",
    "    \"I absolutely love working with Spark Streaming! It's amazing and powerful!\",\n",
    "    \"Kafka integration is fantastic and makes everything so much easier\",\n",
    "    \"This distributed system is incredibly efficient and reliable\",\n",
    "    \"Great tools for big data processing, highly recommended!\",\n",
    "    \"Excellent performance with Elasticsearch indexing, very impressed\",\n",
    "    \n",
    "    # Neutras\n",
    "    \"Apache Spark processes data using structured streaming\",\n",
    "    \"Kafka is a distributed message broker for event streaming\",\n",
    "    \"Elasticsearch provides indexing and search capabilities\",\n",
    "    \"The pipeline consists of producer consumer and streaming components\",\n",
    "    \"Docker containers run the infrastructure services\",\n",
    "    \n",
    "    # Negativas\n",
    "    \"Configuration is frustrating and takes too much time\",\n",
    "    \"Terrible documentation, very difficult to understand\",\n",
    "    \"Performance is poor and disappointing with large datasets\",\n",
    "    \"This setup is awful and causes many problems\",\n",
    "    \"I hate dealing with version compatibility issues\",\n",
    "    \n",
    "    # Mistas\n",
    "    \"Spark is powerful but the learning curve is steep and challenging\",\n",
    "    \"Good results but the process is slow and requires patience\",\n",
    "    \"Effective solution despite some annoying configuration issues\"\n",
    "]\n",
    "\n",
    "class SentimentProducer(SocialMediaProducer):\n",
    "    \"\"\"Producer que envia mensagens com sentimentos variados\"\"\"\n",
    "    \n",
    "    def generate_message(self):\n",
    "        \"\"\"Gera mensagem do dataset de sentimentos\"\"\"\n",
    "        return {\n",
    "            \"user\": random.choice(SAMPLE_USERS),\n",
    "            \"text\": random.choice(SENTIMENT_MESSAGES),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"platform\": \"SimulatedSocial\",\n",
    "            \"message_id\": self.message_count\n",
    "        }\n",
    "\n",
    "print(\"‚úì Producer com mensagens de sentimento variado criado\")\n",
    "print(f\"  - Total de templates: {len(SENTIMENT_MESSAGES)}\")\n",
    "print(f\"  - Positivas: ~6 | Neutras: ~5 | Negativas: ~5 | Mistas: ~3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385cc115",
   "metadata": {},
   "source": [
    "### 8.4.3 Producer com Mensagens Variadas (Sentimento Misto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20029109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Inicializar analisador\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Exemplos de teste\n",
    "test_messages = [\n",
    "    \"Apache Spark is amazing for big data processing!\",\n",
    "    \"Learning distributed systems is challenging but rewarding\",\n",
    "    \"This pipeline is terrible and doesn't work at all\",\n",
    "    \"Kafka provides reliable message streaming capabilities\",\n",
    "    \"I hate dealing with configuration files\"\n",
    "]\n",
    "\n",
    "print(\"=== Teste de An√°lise de Sentimentos ===\\n\")\n",
    "\n",
    "for msg in test_messages:\n",
    "    scores = analyzer.polarity_scores(msg)\n",
    "    \n",
    "    # Classificar baseado no score composto\n",
    "    if scores['compound'] >= 0.05:\n",
    "        sentiment = \"POSITIVO üòä\"\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        sentiment = \"NEGATIVO üòû\"\n",
    "    else:\n",
    "        sentiment = \"NEUTRO üòê\"\n",
    "    \n",
    "    print(f\"Mensagem: {msg[:60]}...\")\n",
    "    print(f\"  Compound: {scores['compound']:.3f}\")\n",
    "    print(f\"  Positivo: {scores['pos']:.3f} | Neutro: {scores['neu']:.3f} | Negativo: {scores['neg']:.3f}\")\n",
    "    print(f\"  ‚Üí Classifica√ß√£o: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f88a7b4",
   "metadata": {},
   "source": [
    "### 8.4.2 Teste de An√°lise de Sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q vaderSentiment\n",
    "\n",
    "print(\"‚úì VADER Sentiment instalado\")\n",
    "print(\"  - Biblioteca: vaderSentiment\")\n",
    "print(\"  - Modelo: L√©xico pr√©-treinado para redes sociais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8352dc",
   "metadata": {},
   "source": [
    "### 8.4.1 Instala√ß√£o VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d511c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8.4 Extens√£o Opcional: An√°lise de Sentimentos (ML)\n",
    "\n",
    "**NOTA:** Esta se√ß√£o √© opcional e demonstra como enriquecer o pipeline com an√°lise de sentimentos usando Machine Learning.\n",
    "\n",
    "### Por que An√°lise de Sentimentos?\n",
    "\n",
    "An√°lise de sentimentos permite:\n",
    "- Classificar mensagens como positivas, neutras ou negativas\n",
    "- Identificar tend√™ncias de sentimento em tempo real\n",
    "- Gerar alertas para sentimentos extremos\n",
    "- Complementar an√°lise de word count com contexto emocional\n",
    "\n",
    "### Biblioteca Utilizada: VADER\n",
    "\n",
    "**VADER (Valence Aware Dictionary and sEntiment Reasoner)**\n",
    "\n",
    "- Desenvolvido especificamente para textos de redes sociais\n",
    "- N√£o requer treinamento (modelo l√©xico pr√©-constru√≠do)\n",
    "- Considera contexto (nega√ß√£o, intensificadores, pontua√ß√£o)\n",
    "- Retorna scores: positivo, negativo, neutro e composto\n",
    "\n",
    "**Refer√™ncia Principal:**\n",
    "> Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
    "> \n",
    "> [Paper Original](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf)\n",
    "\n",
    "**Diferencial deste Trabalho:**\n",
    "- Integra√ß√£o com pipeline Spark Streaming (n√£o batch)\n",
    "- Indexa√ß√£o simult√¢nea de word count + sentimentos no Elasticsearch\n",
    "- Visualiza√ß√£o de sentimentos no Kibana em tempo real\n",
    "- Implementa√ß√£o autocontida no notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf81cc7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Conclus√µes e Observa√ß√µes\n",
    "\n",
    "### 9.1 Objetivos Alcan√ßados\n",
    "\n",
    "‚úÖ **Entrada via Kafka:**\n",
    "- Producer simulando mensagens de rede social implementado\n",
    "- Justificativa documentada para n√£o uso de Discord (OAuth, conex√£o persistente, reprodutibilidade)\n",
    "- Alternativa baseada em gera√ß√£o sint√©tica de dados com caracter√≠sticas realistas\n",
    "\n",
    "‚úÖ **Pipeline Spark Structured Streaming:**\n",
    "- Leitura de stream do Kafka com deserializa√ß√£o JSON\n",
    "- Processamento de word count com janelas temporais (30s/10s slide)\n",
    "- Watermark configurado para lidar com eventos atrasados\n",
    "- Publica√ß√£o de resultados em t√≥pico Kafka de sa√≠da\n",
    "\n",
    "‚úÖ **Integra√ß√£o com Elasticsearch:**\n",
    "- √çndice criado com mapping otimizado\n",
    "- Consumer Kafka indexando resultados em tempo real\n",
    "- Bulk indexing para performance\n",
    "\n",
    "‚úÖ **Visualiza√ß√£o no Kibana:**\n",
    "- Instru√ß√µes detalhadas para cria√ß√£o de Tag Cloud\n",
    "- Alternativas documentadas (Bar Chart, Data Table, Treemap)\n",
    "- Dashboard com m√∫ltiplas visualiza√ß√µes sugeridas\n",
    "\n",
    "‚úÖ **Execu√ß√£o Autocontida:**\n",
    "- **100% das opera√ß√µes executadas em c√©lulas do notebook**\n",
    "- Nenhuma depend√™ncia de scripts externos\n",
    "- Infraestrutura gerenciada via docker-compose\n",
    "\n",
    "### 9.2 Arquitetura Implementada\n",
    "\n",
    "```\n",
    "[Producer Python]  ‚Üí  [Kafka: social-input]  ‚Üí  [Spark Streaming]  ‚Üí  [Kafka: wordcount-output]\n",
    "                                                        ‚Üì\n",
    "                                                  [Console Debug]\n",
    "                                                        \n",
    "[ES Consumer]  ‚Üê  [Kafka: wordcount-output]\n",
    "      ‚Üì\n",
    "[Elasticsearch: wordcount-realtime index]\n",
    "      ‚Üì\n",
    "[Kibana Dashboard: Tag Cloud + Metrics]\n",
    "```\n",
    "\n",
    "### 9.3 Par√¢metros de Configura√ß√£o\n",
    "\n",
    "| Componente | Par√¢metro | Valor |\n",
    "|------------|-----------|-------|\n",
    "| Producer | Taxa de mensagens | 3 msgs/seg |\n",
    "| Producer | Dura√ß√£o | 180 segundos |\n",
    "| Spark | Janela temporal | 30 segundos |\n",
    "| Spark | Slide | 10 segundos |\n",
    "| Spark | Watermark | 1 minuto |\n",
    "| Kafka | Parti√ß√µes (input) | 3 |\n",
    "| Kafka | Parti√ß√µes (output) | 3 |\n",
    "| ES Consumer | Batch size | 30 documentos |\n",
    "\n",
    "### 9.4 Melhorias Poss√≠veis\n",
    "\n",
    "- **An√°lise de Sentimentos:** Integrar modelo VADER ou TextBlob para classificar mensagens\n",
    "- **Filtro de Stop Words:** Remover palavras comuns (the, and, is) para resultados mais relevantes\n",
    "- **Agrega√ß√µes M√∫ltiplas:** Word count por usu√°rio, por per√≠odo do dia, etc.\n",
    "- **Alertas:** Configurar alertas no Kibana para palavras espec√≠ficas\n",
    "- **Persist√™ncia:** Kafka com replica√ß√£o para toler√¢ncia a falhas\n",
    "\n",
    "### 9.5 Refer√™ncias\n",
    "\n",
    "- [Apache Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Kafka Documentation](https://kafka.apache.org/documentation/)\n",
    "- [Elasticsearch Python Client](https://elasticsearch-py.readthedocs.io/)\n",
    "- [Kibana Visualizations](https://www.elastic.co/guide/en/kibana/current/dashboard.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook criado por:** Edilberto Cantuaria  \n",
    "**Data:** 29 de Novembro de 2025  \n",
    "**Disciplina:** PSPD - UnB"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
