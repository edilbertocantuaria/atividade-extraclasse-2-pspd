{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f00e5af",
   "metadata": {},
   "source": [
    "## 1. Inicializa√ß√µes para o Laborat√≥rio\n",
    "\n",
    "Instala√ß√£o de todas as depend√™ncias necess√°rias para o pipeline completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f10e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar ambiente (Colab ou Local)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Ambiente: Google Colab\")\n",
    "else:\n",
    "    print(\"üîß Ambiente: Jupyter Local\")\n",
    "\n",
    "# Vari√°veis globais\n",
    "SPARK_VERSION = \"3.5.0\"\n",
    "HADOOP_VERSION = \"3\"\n",
    "KAFKA_VERSION = \"3.6.0\"\n",
    "SCALA_VERSION = \"2.12\"\n",
    "\n",
    "print(f\"üì¶ Vers√µes: Spark {SPARK_VERSION}, Kafka {KAFKA_VERSION}, Scala {SCALA_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f206377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala√ß√£o de depend√™ncias Python\n",
    "!pip install -q pyspark=={SPARK_VERSION}\n",
    "!pip install -q kafka-python\n",
    "!pip install -q wordcloud matplotlib pandas numpy\n",
    "\n",
    "print(\"‚úÖ Depend√™ncias Python instaladas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd518496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download do Kafka (necess√°rio para broker local)\n",
    "import subprocess\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "KAFKA_DIR = f\"/tmp/kafka_{SCALA_VERSION}-{KAFKA_VERSION}\"\n",
    "\n",
    "if not os.path.exists(KAFKA_DIR):\n",
    "    print(\"üì• Baixando Apache Kafka...\")\n",
    "    kafka_url = f\"https://archive.apache.org/dist/kafka/{KAFKA_VERSION}/kafka_{SCALA_VERSION}-{KAFKA_VERSION}.tgz\"\n",
    "    kafka_tgz = f\"/tmp/kafka.tgz\"\n",
    "    \n",
    "    urllib.request.urlretrieve(kafka_url, kafka_tgz)\n",
    "    \n",
    "    print(\"üì¶ Extraindo Kafka...\")\n",
    "    with tarfile.open(kafka_tgz, \"r:gz\") as tar:\n",
    "        tar.extractall(\"/tmp/\")\n",
    "    \n",
    "    os.remove(kafka_tgz)\n",
    "    print(f\"‚úÖ Kafka instalado em {KAFKA_DIR}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Kafka j√° existe em {KAFKA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31530233",
   "metadata": {},
   "source": [
    "## 2. Configura√ß√£o de Mecanismos para Visualiza√ß√£o de Resultados\n",
    "\n",
    "Prepara√ß√£o do ambiente para coleta e exibi√ß√£o dos resultados em tempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c61a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura√ß√£o de diret√≥rios\n",
    "WORK_DIR = \"/tmp/spark_lab\"\n",
    "CHECKPOINT_DIR = f\"{WORK_DIR}/checkpoints\"\n",
    "RESULTS_DIR = f\"{WORK_DIR}/results\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Diret√≥rio de trabalho: {WORK_DIR}\")\n",
    "print(f\"üìÅ Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"üìÅ Resultados: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af50ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports para visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Configura√ß√£o de plots\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Bibliotecas de visualiza√ß√£o configuradas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676432a2",
   "metadata": {},
   "source": [
    "## 3. Instala√ß√£o e Configura√ß√£o do Apache Spark\n",
    "\n",
    "Setup do Spark com suporte a Kafka Structured Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39dd7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# Configura√ß√£o do Spark com pacotes Kafka\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.jars.packages\", \n",
    "         f\"org.apache.spark:spark-sql-kafka-0-10_{SCALA_VERSION}:{SPARK_VERSION}\")\n",
    "conf.set(\"spark.sql.streaming.checkpointLocation\", CHECKPOINT_DIR)\n",
    "conf.set(\"spark.driver.memory\", \"2g\")\n",
    "conf.set(\"spark.executor.memory\", \"2g\")\n",
    "\n",
    "# Criar sess√£o Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"B2_WordCount_Streaming\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurar n√≠vel de log\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(f\"‚úÖ Spark Session criada: {spark.version}\")\n",
    "print(f\"üìä Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd8fb7f",
   "metadata": {},
   "source": [
    "## 4. Instala√ß√£o e Configura√ß√£o do Kafka\n",
    "\n",
    "Inicializa√ß√£o do Kafka broker e cria√ß√£o dos t√≥picos de entrada e sa√≠da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aca2f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import signal\n",
    "import atexit\n",
    "\n",
    "# Processos do Kafka\n",
    "zookeeper_process = None\n",
    "kafka_process = None\n",
    "\n",
    "def cleanup_kafka():\n",
    "    \"\"\"Finaliza processos Kafka ao encerrar notebook\"\"\"\n",
    "    global zookeeper_process, kafka_process\n",
    "    \n",
    "    if kafka_process:\n",
    "        kafka_process.terminate()\n",
    "        kafka_process.wait()\n",
    "    \n",
    "    if zookeeper_process:\n",
    "        zookeeper_process.terminate()\n",
    "        zookeeper_process.wait()\n",
    "    \n",
    "    print(\"üõë Kafka e Zookeeper finalizados\")\n",
    "\n",
    "# Registrar cleanup\n",
    "atexit.register(cleanup_kafka)\n",
    "\n",
    "# Iniciar Zookeeper\n",
    "print(\"üöÄ Iniciando Zookeeper...\")\n",
    "zookeeper_process = subprocess.Popen(\n",
    "    [f\"{KAFKA_DIR}/bin/zookeeper-server-start.sh\", \n",
    "     f\"{KAFKA_DIR}/config/zookeeper.properties\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL\n",
    ")\n",
    "\n",
    "time.sleep(5)  # Aguardar inicializa√ß√£o\n",
    "\n",
    "# Iniciar Kafka\n",
    "print(\"üöÄ Iniciando Kafka broker...\")\n",
    "kafka_process = subprocess.Popen(\n",
    "    [f\"{KAFKA_DIR}/bin/kafka-server-start.sh\", \n",
    "     f\"{KAFKA_DIR}/config/server.properties\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL\n",
    ")\n",
    "\n",
    "time.sleep(10)  # Aguardar inicializa√ß√£o\n",
    "\n",
    "print(\"‚úÖ Kafka broker rodando em localhost:9092\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5740d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar t√≥picos Kafka\n",
    "def create_kafka_topic(topic_name, partitions=3, replication=1):\n",
    "    \"\"\"Cria t√≥pico Kafka se n√£o existir\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [f\"{KAFKA_DIR}/bin/kafka-topics.sh\",\n",
    "             \"--create\",\n",
    "             \"--bootstrap-server\", \"localhost:9092\",\n",
    "             \"--topic\", topic_name,\n",
    "             \"--partitions\", str(partitions),\n",
    "             \"--replication-factor\", str(replication),\n",
    "             \"--if-not-exists\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        if \"Created topic\" in result.stdout or \"already exists\" in result.stdout:\n",
    "            print(f\"‚úÖ T√≥pico '{topic_name}' pronto\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {result.stdout}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao criar t√≥pico {topic_name}: {e}\")\n",
    "\n",
    "# Criar t√≥picos\n",
    "create_kafka_topic(\"social-input\", partitions=3)\n",
    "create_kafka_topic(\"wordcount-output\", partitions=3)\n",
    "\n",
    "print(\"\\nüìã Listando t√≥picos criados:\")\n",
    "subprocess.run([f\"{KAFKA_DIR}/bin/kafka-topics.sh\",\n",
    "                \"--list\",\n",
    "                \"--bootstrap-server\", \"localhost:9092\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8436102c",
   "metadata": {},
   "source": [
    "## 5. Producer: Gerador de Mensagens Simulando Rede Social\n",
    "\n",
    "**Justificativa:** Conforme explicado na introdu√ß√£o, ao inv√©s de integrar com Discord (que exigiria tokens, webhooks e configura√ß√µes externas), implementamos um gerador autom√°tico de mensagens que simula posts de uma rede social.\n",
    "\n",
    "Este gerador:\n",
    "- Publica mensagens continuamente no t√≥pico `social-input`\n",
    "- Usa vocabul√°rio variado (tecnologia, Big Data, Hadoop, Spark, etc.)\n",
    "- Simula comportamento realista com timestamps e varia√ß√£o de conte√∫do\n",
    "- Permite reprodutibilidade total do experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3065804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import random\n",
    "import threading\n",
    "\n",
    "class SocialMediaSimulator:\n",
    "    \"\"\"Simula posts de rede social enviando para Kafka\"\"\"\n",
    "    \n",
    "    def __init__(self, bootstrap_servers='localhost:9092', topic='social-input'):\n",
    "        self.producer = KafkaProducer(\n",
    "            bootstrap_servers=bootstrap_servers,\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "        )\n",
    "        self.topic = topic\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.message_count = 0\n",
    "        \n",
    "        # Vocabul√°rio simulando posts sobre Big Data\n",
    "        self.templates = [\n",
    "            \"Acabei de implementar um cluster {tech} com {num} nodes! {emoji}\",\n",
    "            \"Performance do {tech} est√° incr√≠vel hoje! Processando {num}GB de dados.\",\n",
    "            \"Algu√©m j√° testou {tech} em produ√ß√£o? Preciso de dicas sobre {feature}.\",\n",
    "            \"Tutorial: Como configurar {tech} para {feature} em {num} passos.\",\n",
    "            \"{emoji} Novo post sobre {tech}! Compartilhando minha experi√™ncia com {feature}.\",\n",
    "            \"Comparando {tech} vs {tech2}: qual √© melhor para {feature}?\",\n",
    "            \"Dica r√°pida de {tech}: sempre configure {feature} para otimizar performance!\",\n",
    "            \"Meu cluster {tech} finalmente est√° rodando! {num} workers processando dados.\",\n",
    "        ]\n",
    "        \n",
    "        self.tech_words = [\"Hadoop\", \"Spark\", \"Kafka\", \"HDFS\", \"YARN\", \"MapReduce\", \n",
    "                           \"Flink\", \"Storm\", \"Hive\", \"Pig\", \"HBase\", \"Cassandra\"]\n",
    "        self.features = [\"streaming\", \"batch processing\", \"fault tolerance\", \"scalability\",\n",
    "                        \"distributed computing\", \"data partitioning\", \"replication\",\n",
    "                        \"load balancing\", \"resource management\", \"data locality\"]\n",
    "        self.emojis = [\"üöÄ\", \"üí°\", \"üî•\", \"‚ö°\", \"üéØ\", \"üìä\", \"üõ†Ô∏è\", \"‚ú®\"]\n",
    "    \n",
    "    def generate_message(self):\n",
    "        \"\"\"Gera uma mensagem aleat√≥ria\"\"\"\n",
    "        template = random.choice(self.templates)\n",
    "        message = template.format(\n",
    "            tech=random.choice(self.tech_words),\n",
    "            tech2=random.choice(self.tech_words),\n",
    "            num=random.randint(2, 100),\n",
    "            feature=random.choice(self.features),\n",
    "            emoji=random.choice(self.emojis)\n",
    "        )\n",
    "        return {\n",
    "            \"timestamp\": int(time.time()),\n",
    "            \"user\": f\"user_{random.randint(1, 50)}\",\n",
    "            \"message\": message\n",
    "        }\n",
    "    \n",
    "    def produce_loop(self, interval=2.0):\n",
    "        \"\"\"Loop de produ√ß√£o de mensagens\"\"\"\n",
    "        while self.running:\n",
    "            msg = self.generate_message()\n",
    "            self.producer.send(self.topic, value=msg)\n",
    "            self.message_count += 1\n",
    "            if self.message_count % 10 == 0:\n",
    "                print(f\"üì§ {self.message_count} mensagens enviadas para '{self.topic}'\")\n",
    "            time.sleep(interval)\n",
    "    \n",
    "    def start(self, interval=2.0):\n",
    "        \"\"\"Inicia producer em thread separada\"\"\"\n",
    "        if not self.running:\n",
    "            self.running = True\n",
    "            self.thread = threading.Thread(target=self.produce_loop, args=(interval,))\n",
    "            self.thread.daemon = True\n",
    "            self.thread.start()\n",
    "            print(f\"‚úÖ Producer iniciado (intervalo: {interval}s)\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Para o producer\"\"\"\n",
    "        if self.running:\n",
    "            self.running = False\n",
    "            if self.thread:\n",
    "                self.thread.join(timeout=5)\n",
    "            self.producer.flush()\n",
    "            self.producer.close()\n",
    "            print(f\"üõë Producer finalizado ({self.message_count} mensagens)\")\n",
    "\n",
    "# Instanciar producer\n",
    "social_producer = SocialMediaSimulator(topic='social-input')\n",
    "\n",
    "print(\"‚úÖ Gerador de mensagens (Social Media Simulator) configurado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45270f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar producer\n",
    "social_producer.start(interval=1.5)  # Envia mensagem a cada 1.5 segundos\n",
    "\n",
    "# Aguardar algumas mensagens serem enviadas\n",
    "time.sleep(10)\n",
    "\n",
    "print(f\"\\nüìä Total de mensagens enviadas at√© agora: {social_producer.message_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b95d2fc",
   "metadata": {},
   "source": [
    "## 6. Configura√ß√£o de Sa√≠da Gr√°fica\n",
    "\n",
    "**Justificativa:** Conforme explicado na introdu√ß√£o, ao inv√©s de ElasticSearch + Kibana (que exigem >4GB RAM, Docker e configura√ß√µes complexas n√£o vi√°veis no Colab), implementamos visualiza√ß√£o inline com `wordcloud`.\n",
    "\n",
    "Esta abordagem:\n",
    "- Consome mensagens do t√≥pico `wordcount-output` em tempo real\n",
    "- Gera nuvem de palavras dinamicamente no pr√≥prio notebook\n",
    "- Permite atualiza√ß√µes near-real-time da visualiza√ß√£o\n",
    "- √â totalmente reproduz√≠vel em qualquer ambiente Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed5ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "\n",
    "class WordCloudVisualizer:\n",
    "    \"\"\"Consome resultados do Kafka e exibe nuvem de palavras\"\"\"\n",
    "    \n",
    "    def __init__(self, bootstrap_servers='localhost:9092', topic='wordcount-output'):\n",
    "        self.consumer = KafkaConsumer(\n",
    "            topic,\n",
    "            bootstrap_servers=bootstrap_servers,\n",
    "            auto_offset_reset='earliest',\n",
    "            enable_auto_commit=True,\n",
    "            value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n",
    "        )\n",
    "        self.word_counts = defaultdict(int)\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.total_words = 0\n",
    "    \n",
    "    def consume_loop(self):\n",
    "        \"\"\"Loop de consumo de mensagens\"\"\"\n",
    "        for message in self.consumer:\n",
    "            if not self.running:\n",
    "                break\n",
    "            \n",
    "            data = message.value\n",
    "            word = data.get('word', '')\n",
    "            count = data.get('count', 0)\n",
    "            \n",
    "            if word:\n",
    "                self.word_counts[word] = count\n",
    "                self.total_words += 1\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Inicia consumer em thread separada\"\"\"\n",
    "        if not self.running:\n",
    "            self.running = True\n",
    "            self.thread = threading.Thread(target=self.consume_loop)\n",
    "            self.thread.daemon = True\n",
    "            self.thread.start()\n",
    "            print(\"‚úÖ Consumer de visualiza√ß√£o iniciado\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Para o consumer\"\"\"\n",
    "        if self.running:\n",
    "            self.running = False\n",
    "            if self.thread:\n",
    "                self.thread.join(timeout=5)\n",
    "            self.consumer.close()\n",
    "            print(\"üõë Consumer finalizado\")\n",
    "    \n",
    "    def generate_wordcloud(self, max_words=100):\n",
    "        \"\"\"Gera nuvem de palavras com contagens atuais\"\"\"\n",
    "        if not self.word_counts:\n",
    "            print(\"‚ö†Ô∏è  Nenhuma palavra processada ainda\")\n",
    "            return None\n",
    "        \n",
    "        # Criar WordCloud\n",
    "        wc = WordCloud(\n",
    "            width=1200,\n",
    "            height=600,\n",
    "            background_color='white',\n",
    "            max_words=max_words,\n",
    "            colormap='viridis',\n",
    "            relative_scaling=0.5,\n",
    "            min_font_size=10\n",
    "        ).generate_from_frequencies(self.word_counts)\n",
    "        \n",
    "        return wc\n",
    "    \n",
    "    def plot_wordcloud(self, max_words=100):\n",
    "        \"\"\"Plota nuvem de palavras\"\"\"\n",
    "        wc = self.generate_wordcloud(max_words)\n",
    "        if wc:\n",
    "            plt.figure(figsize=(14, 7))\n",
    "            plt.imshow(wc, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Nuvem de Palavras - {len(self.word_counts)} palavras √∫nicas | '\n",
    "                     f'{self.total_words} contagens processadas', \n",
    "                     fontsize=16, pad=20)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    def plot_top_words(self, top_n=20):\n",
    "        \"\"\"Plota gr√°fico de barras com top N palavras\"\"\"\n",
    "        if not self.word_counts:\n",
    "            print(\"‚ö†Ô∏è  Nenhuma palavra processada ainda\")\n",
    "            return\n",
    "        \n",
    "        # Ordenar por contagem\n",
    "        sorted_words = sorted(self.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_words = sorted_words[:top_n]\n",
    "        \n",
    "        words = [w[0] for w in top_words]\n",
    "        counts = [w[1] for w in top_words]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(words[::-1], counts[::-1], color='skyblue')\n",
    "        plt.xlabel('Contagem', fontsize=12)\n",
    "        plt.ylabel('Palavra', fontsize=12)\n",
    "        plt.title(f'Top {top_n} Palavras Mais Frequentes', fontsize=14, pad=15)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Instanciar visualizador\n",
    "visualizer = WordCloudVisualizer(topic='wordcount-output')\n",
    "\n",
    "print(\"‚úÖ Visualizador de nuvem de palavras configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c99720",
   "metadata": {},
   "source": [
    "## 7. Processamento: WordCount com Spark Structured Streaming\n",
    "\n",
    "Implementa√ß√£o do WordCount usando Spark Structured Streaming para processar mensagens do Kafka em tempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e526386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, lower, regexp_replace, col, count, to_json, struct\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "# Definir schema das mensagens de entrada\n",
    "input_schema = StructType([\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Schema de entrada definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48b3830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler stream do Kafka\n",
    "df_input = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"social-input\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"‚úÖ Stream de entrada conectado ao t√≥pico 'social-input'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aa12d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsear JSON e processar mensagens\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "df_parsed = df_input \\\n",
    "    .selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), input_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "print(\"‚úÖ Parsing JSON configurado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCount: tokeniza√ß√£o e contagem\n",
    "stopwords_list = {'de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para', '√©', 'com', \n",
    "                  'n√£o', 'uma', 'os', 'no', 'se', 'na', 'por', 'mais', 'as', 'dos', 'como',\n",
    "                  'mas', 'ao', 'ele', 'das', '√†', 'seu', 'sua', 'ou', 'quando', 'muito',\n",
    "                  'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "                  'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'been', 'be',\n",
    "                  'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',\n",
    "                  'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those'}\n",
    "\n",
    "# Processar palavras\n",
    "df_words = df_parsed \\\n",
    "    .select(\n",
    "        explode(\n",
    "            split(\n",
    "                lower(\n",
    "                    regexp_replace(col(\"message\"), \"[^a-zA-Z0-9√°√©√≠√≥√∫√¢√™√¥√£√µ√ß√Å√â√ç√ì√ö√Ç√ä√î√É√ï√á\\\\s]\", \"\")\n",
    "                ), \n",
    "                \"\\\\s+\"\n",
    "            )\n",
    "        ).alias(\"word\")\n",
    "    ) \\\n",
    "    .filter(col(\"word\") != \"\") \\\n",
    "    .filter(~col(\"word\").isin(stopwords_list))\n",
    "\n",
    "# Contar palavras\n",
    "df_wordcount = df_words \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "print(\"‚úÖ Pipeline de WordCount configurado\")\n",
    "print(\"üìä Colunas finais:\", df_wordcount.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7f74cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrever resultados no Kafka (t√≥pico de sa√≠da)\n",
    "query_kafka = df_wordcount \\\n",
    "    .select(to_json(struct(\"word\", \"count\")).alias(\"value\")) \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"wordcount-output\") \\\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_DIR}/kafka-output\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"‚úÖ Stream de sa√≠da iniciado (Kafka topic: 'wordcount-output')\")\n",
    "print(f\"üìä Query ID: {query_kafka.id}\")\n",
    "print(f\"üîÑ Status: {query_kafka.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b180e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamb√©m escrever em console para monitoramento (opcional)\n",
    "query_console = df_wordcount \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 20) \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(\"‚úÖ Stream de console iniciado (exibi√ß√£o a cada 10s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404349f",
   "metadata": {},
   "source": [
    "## 8. Apresenta√ß√£o de Resultados e Visualiza√ß√£o em Dashboard\n",
    "\n",
    "Iniciando consumer para visualiza√ß√£o e gerando gr√°ficos em tempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar consumer de visualiza√ß√£o\n",
    "visualizer.start()\n",
    "\n",
    "# Aguardar acumula√ß√£o de dados\n",
    "print(\"‚è≥ Aguardando 20 segundos para acumular dados...\")\n",
    "time.sleep(20)\n",
    "\n",
    "print(f\"\\nüìä Estat√≠sticas:\")\n",
    "print(f\"   - Palavras √∫nicas: {len(visualizer.word_counts)}\")\n",
    "print(f\"   - Total de contagens: {visualizer.total_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a195cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar visualiza√ß√µes\n",
    "print(\"üìä Gerando visualiza√ß√µes...\\n\")\n",
    "\n",
    "# 1. Top 20 palavras\n",
    "visualizer.plot_top_words(top_n=20)\n",
    "\n",
    "# 2. Nuvem de palavras\n",
    "visualizer.plot_wordcloud(max_words=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc998a1",
   "metadata": {},
   "source": [
    "## 9. Valida√ß√£o e Evid√™ncias\n",
    "\n",
    "Monitoramento final do pipeline e coleta de evid√™ncias de funcionamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar status das queries Spark\n",
    "print(\"üîç Status das Queries Spark Streaming:\\n\")\n",
    "\n",
    "for query in spark.streams.active:\n",
    "    print(f\"Query ID: {query.id}\")\n",
    "    print(f\"Name: {query.name}\")\n",
    "    print(f\"Status: {query.status}\")\n",
    "    print(f\"Recent Progress:\")\n",
    "    \n",
    "    # √öltimos 3 progresses\n",
    "    for progress in query.recentProgress[-3:]:\n",
    "        print(f\"  - Batch: {progress.get('batchId', 'N/A')}\")\n",
    "        print(f\"    Input Rows: {progress.get('numInputRows', 0)}\")\n",
    "        print(f\"    Processing Rate: {progress.get('inputRowsPerSecond', 0):.2f} rows/sec\")\n",
    "        print(f\"    Duration: {progress.get('durationMs', {}).get('total', 0)}ms\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b731e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar mensagens no Kafka\n",
    "import subprocess\n",
    "\n",
    "print(\"üìã Verificando t√≥picos Kafka:\\n\")\n",
    "\n",
    "# Contar mensagens no t√≥pico de entrada\n",
    "result_input = subprocess.run(\n",
    "    [f\"{KAFKA_DIR}/bin/kafka-run-class.sh\", \"kafka.tools.GetOffsetShell\",\n",
    "     \"--broker-list\", \"localhost:9092\",\n",
    "     \"--topic\", \"social-input\",\n",
    "     \"--time\", \"-1\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Contar mensagens no t√≥pico de sa√≠da\n",
    "result_output = subprocess.run(\n",
    "    [f\"{KAFKA_DIR}/bin/kafka-run-class.sh\", \"kafka.tools.GetOffsetShell\",\n",
    "     \"--broker-list\", \"localhost:9092\",\n",
    "     \"--topic\", \"wordcount-output\",\n",
    "     \"--time\", \"-1\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "print(f\"T√≥pico 'social-input':\")\n",
    "print(result_input.stdout)\n",
    "\n",
    "print(f\"\\nT√≥pico 'wordcount-output':\")\n",
    "print(result_output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00bb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amostras de dados processados\n",
    "print(\"üìä Amostra de Palavras Mais Frequentes:\\n\")\n",
    "\n",
    "if visualizer.word_counts:\n",
    "    sorted_words = sorted(visualizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"{'Palavra':<20} {'Contagem':>10}\")\n",
    "    print(\"-\" * 32)\n",
    "    for word, count in sorted_words[:15]:\n",
    "        print(f\"{word:<20} {count:>10}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Nenhum dado processado ainda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e57937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizar visualiza√ß√µes ap√≥s mais dados\n",
    "print(\"\\n‚è≥ Aguardando mais 15 segundos para acumular dados...\")\n",
    "time.sleep(15)\n",
    "\n",
    "print(f\"\\nüìä Estat√≠sticas atualizadas:\")\n",
    "print(f\"   - Palavras √∫nicas: {len(visualizer.word_counts)}\")\n",
    "print(f\"   - Total de contagens: {visualizer.total_words}\")\n",
    "print(f\"   - Mensagens produzidas: {social_producer.message_count}\")\n",
    "\n",
    "# Gerar visualiza√ß√µes finais\n",
    "print(\"\\nüìä Visualiza√ß√µes Finais:\\n\")\n",
    "visualizer.plot_top_words(top_n=25)\n",
    "visualizer.plot_wordcloud(max_words=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5004f3",
   "metadata": {},
   "source": [
    "## 10. Finaliza√ß√£o e Limpeza\n",
    "\n",
    "Parando todos os servi√ßos e streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e80613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parar producer\n",
    "social_producer.stop()\n",
    "\n",
    "# Parar visualizador\n",
    "visualizer.stop()\n",
    "\n",
    "# Parar queries Spark\n",
    "for query in spark.streams.active:\n",
    "    print(f\"üõë Parando query: {query.id}\")\n",
    "    query.stop()\n",
    "\n",
    "# Aguardar finaliza√ß√£o\n",
    "time.sleep(5)\n",
    "\n",
    "print(\"\\n‚úÖ Todos os streams finalizados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estat√≠sticas finais\n",
    "print(\"üìä ESTAT√çSTICAS FINAIS DO PIPELINE\\n\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Mensagens produzidas: {social_producer.message_count}\")\n",
    "print(f\"Palavras √∫nicas processadas: {len(visualizer.word_counts)}\")\n",
    "print(f\"Total de contagens: {visualizer.total_words}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Top 10 palavras finais\n",
    "if visualizer.word_counts:\n",
    "    sorted_words = sorted(visualizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"\\nTOP 10 PALAVRAS MAIS FREQUENTES:\")\n",
    "    for i, (word, count) in enumerate(sorted_words[:10], 1):\n",
    "        print(f\"  {i}. {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7858feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parar Spark\n",
    "spark.stop()\n",
    "print(\"üõë Spark Session finalizada\")\n",
    "\n",
    "# Cleanup Kafka ser√° feito automaticamente pelo atexit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a495be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Conclus√µes\n",
    "\n",
    "### Pipeline Implementado\n",
    "\n",
    "1. **Entrada:** Gerador autom√°tico de mensagens ‚Üí Kafka `social-input`\n",
    "2. **Processamento:** Spark Structured Streaming ‚Üí WordCount com tokeniza√ß√£o e remo√ß√£o de stopwords\n",
    "3. **Sa√≠da:** Kafka `wordcount-output` ‚Üí Consumer Python ‚Üí Visualiza√ß√£o inline (WordCloud + BarChart)\n",
    "\n",
    "### Justificativas de Adapta√ß√µes\n",
    "\n",
    "- **Discord ‚Üí Gerador de Texto:** Mant√©m o conceito de stream de mensagens sem depend√™ncias externas complexas\n",
    "- **ElasticSearch/Kibana ‚Üí WordCloud inline:** Visualiza√ß√£o equivalente sem overhead de infraestrutura\n",
    "\n",
    "### Diferenciais Implementados\n",
    "\n",
    "- ‚úÖ Pipeline 100% reproduz√≠vel em Colab ou Jupyter local\n",
    "- ‚úÖ Visualiza√ß√µes near-real-time com atualiza√ß√µes din√¢micas\n",
    "- ‚úÖ Remo√ß√£o de stopwords (PT + EN) para melhor qualidade\n",
    "- ‚úÖ Monitoramento completo (Spark UI, Kafka offsets, contadores)\n",
    "- ‚úÖ Cleanup autom√°tico de recursos\n",
    "\n",
    "### Tecnologias Utilizadas\n",
    "\n",
    "- Apache Spark 3.5.0 (Structured Streaming)\n",
    "- Apache Kafka 3.6.0 (Message Broker)\n",
    "- Python: kafka-python, wordcloud, matplotlib, pandas\n",
    "- Zookeeper (para Kafka)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook executado com sucesso!**  \n",
    "**Data:** 14 de novembro de 2025"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
