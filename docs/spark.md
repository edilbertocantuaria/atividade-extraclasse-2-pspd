# DocumentaÃ§Ã£o TÃ©cnica - Spark

> ğŸš€ **Para instruÃ§Ãµes de execuÃ§Ã£o**, consulte **[como_executar.md](../como_executar.md)**

> Esta documentaÃ§Ã£o contÃ©m detalhes tÃ©cnicos sobre arquitetura e configuraÃ§Ãµes Spark Streaming.

## ğŸ—ï¸ Arquitetura

### Componentes do Ambiente

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Producer   â”‚â”€â”€â”€â”€â–¶â”‚    Kafka     â”‚â”€â”€â”€â”€â–¶â”‚    Spark     â”‚
â”‚   (Python)   â”‚     â”‚   (Broker)   â”‚     â”‚  (Streaming) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                                  â–¼
                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â”‚ Elasticsearch   â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                                  â–¼
                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â”‚     Kibana      â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Containers

| ServiÃ§o | Porta | FunÃ§Ã£o |
|---------|-------|--------|
| **Zookeeper** | 2181 | CoordenaÃ§Ã£o Kafka |
| **Kafka** | 9092 | Message broker |
| **Elasticsearch** | 9200, 9300 | Armazenamento e busca |
| **Kibana** | 5601 | VisualizaÃ§Ã£o |
| **Spark Master** | 7077, 8080 | CoordenaÃ§Ã£o cluster Spark |
| **Spark Worker** | - | ExecuÃ§Ã£o de tasks |

## âš™ï¸ ConfiguraÃ§Ãµes

### Docker Compose

O ambiente completo Ã© orquestrado via `docker-compose.yml`:

```yaml
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:latest
    ports:
      - "9092:9092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    ports:
      - "9200:9200"
    environment:
      discovery.type: single-node
      xpack.security.enabled: false

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    ports:
      - "5601:5601"

  spark-master:
    build: .
    ports:
      - "8080:8080"
      - "7077:7077"
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
```

### Spark Streaming

ConfiguraÃ§Ã£o do streaming (micro-batches de 5 segundos):

```python
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils

sc = SparkContext(appName="WordCount")
ssc = StreamingContext(sc, 5)  # 5 segundos

# Conectar ao Kafka
kafkaStream = KafkaUtils.createDirectStream(
    ssc,
    ["input-topic"],
    {"bootstrap.servers": "kafka:9092"}
)
```

## ğŸš€ Uso

### Iniciar Ambiente

```bash
cd spark
docker compose up -d
```

### Verificar Status

```bash
# Verificar containers
docker compose ps

# Logs
docker compose logs -f spark-master
docker compose logs -f kafka
```

### Acessar Interfaces

- **Spark UI**: http://localhost:8080
- **Kibana**: http://localhost:5601
- **Elasticsearch**: http://localhost:9200
- **Jupyter** (se configurado): http://localhost:8888

### Executar AplicaÃ§Ã£o Spark

```bash
# Dentro do container
docker exec -it spark-master bash

# Submeter job
spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.0 \
  /opt/spark_app/wordcount_streaming.py
```

### Produzir Mensagens Kafka

```bash
# Via Python producer
docker exec -it spark-master python3 /opt/spark_app/producer.py

# Via Kafka console
docker exec -it kafka kafka-console-producer \
  --broker-list localhost:9092 \
  --topic input-topic
```

## ğŸ§ª Testes

### 1. Teste de Conectividade

```bash
# Testar Kafka
docker exec kafka kafka-topics \
  --list \
  --bootstrap-server localhost:9092

# Testar Elasticsearch
curl http://localhost:9200/_cluster/health

# Testar Kibana
curl http://localhost:5601/api/status
```

### 2. Teste de Streaming

```bash
# Terminal 1: Iniciar aplicaÃ§Ã£o Spark
docker exec -it spark-master spark-submit /opt/spark_app/wordcount_streaming.py

# Terminal 2: Enviar dados
docker exec -it spark-master python3 /opt/spark_app/producer.py

# Terminal 3: Verificar Elasticsearch
curl "http://localhost:9200/wordcount/_search?pretty"
```

### 3. VisualizaÃ§Ã£o no Kibana

1. Acesse http://localhost:5601
2. Management â†’ Index Patterns â†’ Create
3. Index pattern: `wordcount*`
4. Discover â†’ Ver dados em tempo real
5. Visualize â†’ Criar grÃ¡ficos

## ğŸ“Š Monitoramento

### Spark

```bash
# Verificar workers ativos
curl http://localhost:8080/json/

# AplicaÃ§Ãµes rodando
curl http://localhost:8080/api/v1/applications
```

### Kafka

```bash
# Consumer groups
docker exec kafka kafka-consumer-groups \
  --list \
  --bootstrap-server localhost:9092

# Offset lag
docker exec kafka kafka-consumer-groups \
  --describe \
  --group spark-streaming \
  --bootstrap-server localhost:9092
```

### Elasticsearch

```bash
# Ãndices
curl http://localhost:9200/_cat/indices?v

# Contar documentos
curl http://localhost:9200/wordcount/_count

# Buscar documentos
curl "http://localhost:9200/wordcount/_search?pretty&size=10"
```

## ğŸ”§ Troubleshooting

### Spark nÃ£o conecta ao Kafka

```bash
# Verificar conectividade
docker exec spark-master ping kafka

# Verificar tÃ³picos
docker exec kafka kafka-topics \
  --list \
  --bootstrap-server localhost:9092

# Criar tÃ³pico manualmente
docker exec kafka kafka-topics \
  --create \
  --topic input-topic \
  --bootstrap-server localhost:9092 \
  --partitions 1 \
  --replication-factor 1
```

### Elasticsearch inacessÃ­vel

```bash
# Verificar logs
docker logs elasticsearch

# Verificar saÃºde
curl http://localhost:9200/_cluster/health

# Reiniciar
docker compose restart elasticsearch
```

### Job Spark falha

```bash
# Ver logs detalhados
docker logs spark-master
docker logs spark-worker

# Verificar recursos
docker stats

# Limpar checkpoints (se necessÃ¡rio)
docker exec spark-master rm -rf /tmp/checkpoint
```

### Kibana nÃ£o carrega dados

```bash
# Verificar Ã­ndice existe
curl http://localhost:9200/_cat/indices?v

# Refresh index pattern no Kibana
# Management â†’ Index Patterns â†’ wordcount* â†’ Refresh field list
```

## ğŸ“ Estrutura do Projeto Spark

```
spark/
â”œâ”€â”€ docker-compose.yml        # OrquestraÃ§Ã£o
â”œâ”€â”€ Dockerfile                # Imagem Spark customizada
â”œâ”€â”€ testar_ambiente.sh        # Script de validaÃ§Ã£o
â”œâ”€â”€ spark_app/
â”‚   â”œâ”€â”€ wordcount_streaming.py  # AplicaÃ§Ã£o principal
â”‚   â”œâ”€â”€ producer.py             # Produtor Kafka
â”‚   â””â”€â”€ requirements.txt        # DependÃªncias Python
â”œâ”€â”€ notebooks/                # Jupyter notebooks (opcional)
â””â”€â”€ elastic/
    â””â”€â”€ kibana_dashboards/    # Dashboards exportados
```

## ğŸ” ConsideraÃ§Ãµes de SeguranÃ§a

**ConfiguraÃ§Ã£o atual Ã© para desenvolvimento:**
- Elasticsearch sem autenticaÃ§Ã£o
- Kafka sem SSL
- Portas expostas localmente

**Para produÃ§Ã£o:**
- Habilitar X-Pack Security (Elasticsearch)
- SSL/TLS no Kafka
- AutenticaÃ§Ã£o e autorizaÃ§Ã£o
- Network policies

## ğŸ“š ReferÃªncias

- [Spark Streaming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
- [Kafka Documentation](https://kafka.apache.org/documentation/)
- [Elasticsearch Guide](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)
- [Kibana Guide](https://www.elastic.co/guide/en/kibana/current/index.html)
